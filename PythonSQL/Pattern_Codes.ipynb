{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d33e82e-fa7b-4685-8020-588f21353e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "insertion_time = dt.datetime.strptime(\"2023/10/20 23:59:55\", '%Y/%m/%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22f20df-75ca-40de-a58b-46edb449c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Split the data into training, validation, and test sets\n",
    "Set up the pipeline with a scaler and a regressor\n",
    "Set up hyperparameters for tuning\n",
    "Set up K-fold cross-validation\n",
    "Set up GridSearchCV for hyperparameter tuning\n",
    "Fit the model using GridSearchCV on the validation set\n",
    "Get the best estimator\n",
    "Get the predictions on the test set using the best estimator\n",
    "Visualize the actual vs predicted values\n",
    "\n",
    "1) One-class SVM (Support Vector Machine) \n",
    "(data preprocessing, feature selection, model training, and evaluation)\n",
    "\n",
    "SelectKBest\n",
    "Split data for test and training\n",
    "Cross-validation\n",
    "Create Pipeline \n",
    "Information about performance of model\n",
    "Visualise Actual and Predictied Value using Seaborn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c76b81-aef1-4bb1-8625-168c8cd464df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_function():\n",
    "    print(\"This is a function in the script.\")\n",
    "\n",
    "# Check if the script is being run as the main program\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"This will only run if the script is executed directly.\")\n",
    "    some_function()\n",
    "else:\n",
    "    print(\"This will run if the script is imported as a module.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5afcf6c-6d3e-4938-a7d7-19343a8a7611",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f0ecee-6ec2-4a34-a3b1-d4ca21698464",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample.csv'   , encoding='cp932', header=0, parse_dates=True, index_col='column', dtype=object,  sheet_name='sheet_name')\n",
    "df = pd.read_excel('sample.xlsx', encoding='cp932', header=0, parse_dates=True, index_col='column', dtype=object,  sheet_name='sheet_name')\n",
    "\n",
    "df.to_csv(\"file.xlsx\",   header = True, index=False, encoding=\"cp932\")\n",
    "df.to_excel(\"file.xlsx\", header = True, index=False, encoding=\"cp932\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6dd30-2f8f-492e-bb88-e6f2e7b6ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = glob.glob(file_path + \"*.csv\")\n",
    "df_list = [pd.read_csv(r\"C:\\Users\\00220401626\\Desktop\\General.csv\", encoding = \"cp932\", index = False) for file in path]\n",
    "df = pd.concat(df_list, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b1487c-13b0-4307-a470-455757451f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(folder_name +\"*.csv\")              \n",
    "dfs = []\n",
    "for file in files:\n",
    "    i = pd.read_csv(file, encoding=\"cp932\", index = False)\n",
    "    dfs.append(i)\n",
    "df = pd.concat(dfs, axis=0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320c35d-5698-4194-8cf7-0ec6999a641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "td=pd.DataFrame()\n",
    "for root, dirs, files in os.walk(r\"C:\\Users\\00220401626\\Desktop\\General.csv\"):\n",
    "    for i in files:\n",
    "        if i.endswith(\".csv\"):\n",
    "        full = os.path.join(root, i)\n",
    "        d=pd.read_csv(full,encoding=\"cp932\")\n",
    "        td=pd.concat([td,d], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf52199e-560f-43c5-ab04-d7af13e31dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T\n",
    "df.head()\n",
    "df.tail()\n",
    "df.info()\n",
    "df.describe()\n",
    "df.shape\n",
    "df.count()\n",
    "df.dtypes\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2275a639-05d8-443d-93e1-9a02df29dc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['column'].min()\n",
    "df['column'].max()\n",
    "df['column'].mean()\n",
    "df['column'].median()\n",
    "df['column'].sum()\n",
    "df['column'].count()                                                                          # Count of non-null values\n",
    "df['column'].diff()                                                                           # Calculate the difference between previous and next \n",
    "df['column'].unique()\n",
    "df['column'].nunique()                                                                        # umber of unique values\n",
    "df['column'].isna().sum()\n",
    "df['column'].isnull().sum()\n",
    "df['column'].notna().sum()\n",
    "df['column'].notnull().sum()\n",
    "df['column'].isin(['value1', 'value2'])\n",
    "df['column'].duplicated().sum()\n",
    "df['column'].tolist()\n",
    "a = df['column'].value_counts()['value1']\n",
    "a = (df['column'] == 'ABS').values.sum()\n",
    "a = df['column'].str.isspace()\n",
    "top_values = df['column'].nlargest(10)                                                        # top 10 values from the 'column'\n",
    "bottom_values = df['column'].nsmallest(10)                                                    # the bottom 10 values from the 'column'\n",
    "cumulative_sum_A = df['A'].cumsum()                                                           # Calculating the cumulative sum for column 'A'\n",
    "shifted_values = df['column'].shift(2)                                                        # Shift values in the 'column' by 1 period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286db134-e38c-4045-9ab7-e4ed6eff3daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Time', inplace=True) \n",
    "df1= df.resample('D').nunique()                                                               # calculate the number of unique values for each day  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5953aea-f56d-4270-9342-829db0d307a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['A', 'B', 'C'], axis = 1, inplace = True)\n",
    "df.dropna(subset=['A'], inplace=True)\n",
    "df.drop_duplicates(subset=['A', 'B'], inplace = True)\n",
    "df.replace(\"Null\",np.nan)\n",
    "df.fillna(0, inplace=True)\n",
    "df[\"A\"].fillna(0, inplace = True)\n",
    "df['B'].fillna(df['B'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa801571-c48d-41be-8766-409b75ec2ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:3]\n",
    "df[:3]\n",
    "df[9:14]\n",
    "df[100:105]\n",
    "df['2020-08-15':'2020-09-14']\n",
    "\n",
    "df[df[\"column\"] == 'a']\n",
    "df[df['column'] == 2015]\n",
    "df[df['column'].isin([2010])]\n",
    "df[df['column'].isin(['A','B'])]\n",
    "df[df['column'].str.contains('a')]\n",
    "df[df['column'].str.startswith('b')]\n",
    "df[df['column'].str.endswith('c')]\n",
    "df[df['Column'] == 'Value'][['Column1', 'Column2']]\n",
    "\n",
    "df['column'].isin(['10']).sum()\n",
    "df[df['column1'] <= df['column2'].max()]\n",
    "df[(df['column_1'] == 2015) & (df['column_2'] == 'A')]\n",
    "df[(df['column_1'] == 2010) | (df['column_2'] == 2015)]\n",
    "df[(df[\"column_1\"] == 'a')  | (df[\"column_2\"] == '10') | (df[\"column_3\"] == '15')]\n",
    "\n",
    "df = df.loc[::step]\n",
    "df = df.loc[:,['column_1','column_2']]\n",
    "df = df.loc[(df['column_1'] >= a) & (df['column_2'] <= b)] \n",
    "df = df.loc[df['column_1'] > 20, ['column_2', 'column_3']]\n",
    "df.iloc[row_indices, column_indices]\n",
    "df = df.iloc[[1, 3], [0, 2]]\n",
    "df.iloc[:, 2] = 'new_column'                                                          # Rename column\n",
    "\n",
    "\n",
    "df[\"column\"] = df[\"column\"].astype(str)\n",
    "df[\"column\"] = df[\"column\"].astype(int)\n",
    "df[\"column\"] = df[\"column\"].astype(float)\n",
    "\n",
    "df['column'] = df['column'].apply(lambda x: x*2)\n",
    "\n",
    "df['column'] = df['column'].str[-15:-4]                                              # belgilangan strni kesib oladi\n",
    "df['column'] = df['column'].str.strip()                                              # Remove each whitespace from column\n",
    "\n",
    "df['City'] = df['City'].replace('Paris', 'Margilon')\n",
    "df = df['ColumnToSplit'].str.split(\" \", expand=True)                                 # bitta column ni ikkiga bo'ladi\n",
    "df[\"column_3\"] = df[\"column_1\"] + \"-\" + df[\"column_2\"]                               # ikkita column ni bittaga jamlash\n",
    "df[\"column_3\"] = df[\"column_1\"].astype(str) + \"-\" + df[\"column_2\"].astype(str)\n",
    "\n",
    "df['A'] = df['A'].str.strip()                                                        # remove whitespaces\n",
    "\n",
    "df = df.rename(columns={'a': 'A'})\n",
    "df.rename(columns={'column_1': '1', 'column_2': '2','column_3':'3'}, inplace=True)   # rename columns\n",
    "df.insert(loc=0, column=\"column_name\", value = str(100))                             # Convert the value to a string and insert it as the first column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc622db-ea70-4733-af1e-adbfe33f268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Time'] = pd.to_datetime(df['Time'], format='%Y-%m:%d %H:%M:%S')                              \n",
    "df['A'] = df['A'].dt.strftime('%Y-%m-%d %H:%M:%S')                                                        # Change datetime to string\n",
    "df['A'] = pd.to_datetime(df['A'],unit='ms').dt.strftime('%H:%M:%S:%f').str[:-3]\n",
    "df['A'] = df['A'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))                              # Change string to datetime\n",
    "\n",
    "df[\"A\"] = pd.to_datetime(df[\"A\"]) + pd.to_timedelta(df[\"A\"]/1000, \"S\")\n",
    "a = timedelta(days = 5, hours = 2, minutes = 30)                                                          # Creating Timedelta\n",
    "a = datetime.datetime(year=2017, month=10, day=10, hour=15)                                               # Cteating datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c08cff4a-90af-441b-9c01-52f3bb5bcd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='column', ascending=False, inplace=True)                                                # Sort by a single column in descending order\n",
    "df.sort_values(by='column', ascending=False, na_position='first', inplace=True)                           # Sort the DataFrame by the values of a specific column in descending order\n",
    "df.sort_values(by=['column_1', 'column_2'], ascending=False, inplace=True)                                # Sort by multiple columns in descending order\n",
    "df.sort_values(by=['column_1', 'column_2', 'column_3'], ascending=[False, True, False], inplace=True)     # Sort by multiple columns with different sorting orders\n",
    "df_sorted = df.sort_values(by=['column']).reset_index(drop=True)                                          # Sort the DataFrame by 'column' and reset the index\n",
    "df.sort_index(ascending=False, inplace=True, na_position='first')                                         # Sort the DataFrame by the index in descending order with na_position specified\n",
    "df.reset_index(inplace=True)                                                                              #  Reset the index of the DataFrame\n",
    "filtered_df = df.loc[df['column'] == 100].reset_index(drop=True)                                          #  Filter rows where 'column' equals a specific value and reset the index\n",
    "df.reindex(columns=['column_1', 'column_2','column_3'], inplace=True)\n",
    "df.set_index('column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed33163-ab48-4bf3-ae12-4f2c352616af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(items=['value_1' , 'value_2'], axis=0))                                                        \n",
    "df.filter(items=['column_1' , 'column_2'], axis=1))                                                      \n",
    "df.filter(items=['column_1', 'column_2']).filter(items=['value_1', 'value_2'], axis=0))                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6251b1b0-9d1e-4663-8e13-fe5fed5e843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby['A'].min()\n",
    "df.groupby['A'].max()\n",
    "df.groupby('A').mean()\n",
    "df.groupby('A').max()\n",
    "df.groupby['A'].median()\n",
    "\n",
    "df['A'] = df.groupby(['B']).cumcount()+1                                      #Sequence number of each element starting from 1\n",
    "df['column_3'] = df.groupby(['column_1','column_2']).cumcount()               #Sequence number of each element starting from 0\n",
    "\n",
    "df[['A','B']].groupby(['C','A']).mean()\n",
    "df[['A','B']].groupby('C').agg(['mean','sum','count','max','min','std','var'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b26d43-f857-4f9b-9afc-1cf98b322a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.resample('H').max()\n",
    "df.resample('10D').sum()\n",
    "df.resample(\"D\").mean()\n",
    "df.resample('D').nunique()\n",
    "df.resample('D').count()\n",
    "df.resample(\"M\").sum()\n",
    "df.resample(\"Q\").agg([\"sum\", \"mean\", \"max\", \"min\"])\n",
    "df.resample('W').agg(['min', 'max', 'sum'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4961c8-2ff7-41ae-9f61-11b667d9e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot(index='...', columns='...', values='...')\n",
    "pd.pivot_table(df, values='...', index='...', columns='...', aggfunc='...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237545c5-e33b-4a4d-b8d6-e6d17d50997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], axis=0, join='outer', ignore_index=True)\n",
    "pd.merge(left_df, right_df, on=...., how=...., left_on=....., right_on=....., left_index=......, right_index=.....)\n",
    "df = df1.join(df2, on='price', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c05d7-bb19-471d-a065-2a71983c2884",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('year == 2010')\n",
    "df.query('year > 2010')\n",
    "df.query('column == A')\n",
    "df.query('column_1 == 2010 or column_2 == 2015')\n",
    "df.query('column == \"A\" and column == 2015')\n",
    "\n",
    "df = pd.RangeIndex(start=1, stop=len(d.index) + 1, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e486e919-90fb-4091-98b7-2fc66811d726",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "           A           B\n",
      "0  123456789  3456789012\n",
      "1  987654321  8765432109\n",
      "\n",
      "Formatted DataFrame:\n",
      "             A              B\n",
      "0  123,456,789  3,456,789,012\n",
      "1  987,654,321  8,765,432,109\n",
      "\n",
      "Formatted DataFrame with Commas Replaced by Spaces:\n",
      "             A              B\n",
      "0  123 456 789  3 456 789 012\n",
      "1  987 654 321  8 765 432 109\n"
     ]
    }
   ],
   "source": [
    "formatted_df = df.applymap('{:,.0f}'.format)                # Apply the formatting using applymap, har 3 ta qatorni vergul bilan ajratadi\n",
    "formatted_df = formatted_df.replace(',', ' ', regex=True)   # Replace commas with spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "186a8dc6-9728-4d17-b680-b01382a99511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numeric_column</th>\n",
       "      <th>binned_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0-50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>0-50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75</td>\n",
       "      <td>51-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110</td>\n",
       "      <td>101-150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160</td>\n",
       "      <td>150+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40</td>\n",
       "      <td>0-50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>90</td>\n",
       "      <td>51-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>130</td>\n",
       "      <td>101-150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   numeric_column binned_column\n",
       "0              20          0-50\n",
       "1              25          0-50\n",
       "2              75        51-100\n",
       "3             110       101-150\n",
       "4             160          150+\n",
       "5              40          0-50\n",
       "6              90        51-100\n",
       "7             130       101-150"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {'numeric_column': [20, 25, 75, 110, 160, 40, 90, 130]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define bins and labels\n",
    "bins = [0, 50, 100, 150, np.inf]\n",
    "labels = ['0-50', '51-100', '101-150', '150+']\n",
    "\n",
    "df['binned_column'] = pd.cut(df['numeric_column'], bins=bins, labels=labels)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4e1a6-eb1e-487e-9144-c1e1cd7e39af",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_csv_files = [file for file in csv_files if int(file[-12:-4]) >= int(inserted_day)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f4b707-cc4d-4347-ae45-5b43c8d0a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hamma CSV Filelarni bitta dataframega joylash\n",
    "path =  r\"C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2023\"\n",
    "csv_files = glob.glob(os.path.join(path, '*.csv'))\n",
    "data_frames = []\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file, encoding='cp932',dtype= object)\n",
    "    data_frames.append(df)\n",
    "concatenated_df = pd.concat(data_frames, ignore_index=True)\n",
    "concatenated_df['Timestamp'] = pd.to_datetime(concatenated_df['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e13ad91-d7bb-490a-a7f9-cdc15d8f63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = []\n",
    "for location in a:\n",
    "    latest_location = df[df['Location'] == location].nlargest(1, 'Timestamp')\n",
    "    df_concat.append(latest_location)\n",
    "latest_data = pd.concat(df_concat, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532fbab9-914a-4795-961f-7ef7fe0ec3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory =  r\"C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\"\n",
    "file_pattern = '20*'  \n",
    "files = glob.glob(os.path.join(directory, file_pattern))\n",
    "files.sort()\n",
    "folder_path = files[-1]\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "latest_csv_file = max(csv_files, key=os.path.getmtime)\n",
    "df2 = pd.read_csv(latest_csv_file, encoding='cp932',dtype= object)\n",
    "df2['Timestamp'] = pd.to_datetime(df2['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e02b3f6-384f-474c-8e7e-f3e280f9ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "json_filename = 'insertion_time.json'\n",
    "json_file_path = os.path.join(current_directory, json_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63461e78-94a8-40ae-b31d-046bf837be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT CSV FROM LOCAL \n",
    "df = pd.read_csv(r\"C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2023\\20231001.csv\", encoding='cp932',dtype= object)\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b18230-cca5-460f-99cd-ffb7fa9a30bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\20231001.csv\", encoding='cp932', dtype=object, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "638157ff-abb9-48bf-a346-bb5f430de734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime_column</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>weekday_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-15 08:30:00</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>08:30:00</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-20 15:45:30</td>\n",
       "      <td>2023-02-20</td>\n",
       "      <td>15:45:30</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>30</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-25 18:20:45</td>\n",
       "      <td>2023-03-25</td>\n",
       "      <td>18:20:45</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>45</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      datetime_column        date      time  year  month  day  hour  minute  \\\n",
       "0 2023-01-15 08:30:00  2023-01-15  08:30:00  2023      1   15     8      30   \n",
       "1 2023-02-20 15:45:30  2023-02-20  15:45:30  2023      2   20    15      45   \n",
       "2 2023-03-25 18:20:45  2023-03-25  18:20:45  2023      3   25    18      20   \n",
       "\n",
       "   second weekday_name  \n",
       "0       0       Sunday  \n",
       "1      30       Monday  \n",
       "2      45     Saturday  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {'datetime_column': ['2023-01-15 08:30:00', '2023-02-20 15:45:30', '2023-03-25 18:20:45']}\n",
    "df = pd.DataFrame(data)\n",
    "df['datetime_column'] = pd.to_datetime(df['datetime_column'])\n",
    "\n",
    "df['date'] = df['datetime_column'].dt.date\n",
    "df['time'] = df['datetime_column'].dt.time\n",
    "df['year'] = df['datetime_column'].dt.year\n",
    "df['month'] = df['datetime_column'].dt.month\n",
    "df['day'] = df['datetime_column'].dt.day\n",
    "df['hour'] = df['datetime_column'].dt.hour\n",
    "df['minute'] = df['datetime_column'].dt.minute\n",
    "df['second'] = df['datetime_column'].dt.second\n",
    "df['weekday_name'] = df['datetime_column'].dt.day_name()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc26709a-7083-4f21-a386-b3da0cf67c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buff['COND_VALUES'] = df_csv[\"1\"].str.lstrip()\n",
    "df_buff['COND_VALUES'] = df_buff['COND_VALUES'].astype(float) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd56ab-56fc-4b05-b63f-9676ab7d85f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "str.split('a')\n",
    "str.replace('this','with this')\n",
    "str.contains('a')\n",
    "str.startswith('b','c')\n",
    "str.endswith('b','c')\n",
    "str.upper()\n",
    "str.lower()\n",
    "-----------------------------------------------------------\n",
    "str.islower()\n",
    "str.isupper()\n",
    "str.capitalize()\n",
    "str.title()\n",
    "------------------------------------------------------------\n",
    "df.iloc[0,0]                # first location\n",
    "df.iloc[0,0] = '22'\n",
    "\n",
    "df.loc['c', 'two']         # row c and column two\n",
    "df.loc['c', 'two'] = 33 \n",
    "\n",
    "df.loc['a':'c']            # rows a to c\n",
    "df.loc['b':'d', 'two']     # rows b to d and column two \n",
    "\n",
    "df.loc[:, 'C':'E']         # all rows from C to E column\n",
    "\n",
    "df.loc[df[\"gender\"] == \"male\",\"gender\"] = 0\n",
    "df.loc[df[\"gender\"] == \"female\",\"gender\"] = 1\n",
    "\n",
    "----------------------------------------------------------\n",
    "df.drop_duplicates(subset=['A'], keep='last')\n",
    "df.drop([0,4], inplace=True)\n",
    "\n",
    "df.dropna()\n",
    "df.dropna(subset = ['C')]\n",
    "df.fillna(1)\n",
    "df.groupby('Group')['ID'].nunique()\n",
    "pd.unique(df['A']).tolist()\n",
    "df[df['A'] == 1]['B'].tolist()\n",
    "\n",
    "-------------------------------------------------------------\n",
    "\n",
    "df['count_B']=df.groupby(['group1','group2']).B.transform('count')\n",
    "df.groupby(['A','B']).mean()\n",
    "df.groupby(['A','B']).agg(['count', 'mean'])\n",
    "for jinsi, data in df.groupby('Jinsi'):\n",
    " data.to_csv(\"{}.csv\".format(jinsi))\n",
    "\n",
    "------------------------------------------------------------\n",
    "df[:10] # same as df.head(10)\n",
    "df[-10:] # same as df.tail(10)\n",
    "\n",
    "-----------------------------------------------------------\n",
    "for index, row in df.iterrows()\n",
    "\n",
    "----------------------------------------------------------\n",
    "list1 = [1, 2, 3, 4]\n",
    "list2 = ['a', 'b', 'c', 'd']\n",
    "list3 = [10, 20, 15, 30]\n",
    "\n",
    "# Use zip to combine the lists\n",
    "zipped = zip(list1, list2, list3)\n",
    "zipped_list = list(zipped)\n",
    "print(zipped_list)\n",
    "\n",
    "---------------------------------------------------------\n",
    "a = [1, 2, 3]\n",
    "b = [4, 5, 6]\n",
    "\n",
    "c = [x+y for (x,y) in zip(a,b)]\n",
    "\n",
    "---------------------------------------------------------\n",
    "\n",
    "zipped_list = [(1, 'a',1), (2, 'b',3), (3, 'c',4), (4, 'd',5)]\n",
    "\n",
    "list1, list2, list3 = zip(*zipped_list)\n",
    "\n",
    "print(list1)\n",
    "print(list2)\n",
    "print(list3)\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "df = pd.DataFrame([{'var1': 'a,b,c', 'var2': 1, 'var3': 'XX'},\n",
    " {'var1': 'd,e,f,x,y', 'var2': 2, 'var3': 'ZZ'}])\n",
    "print(df)\n",
    "reshaped = \\\n",
    "(df.set_index(df.columns.drop('var1',1).tolist())\n",
    " .var1.str.split(',', expand=True)\n",
    " .stack()\n",
    " .reset_index()\n",
    " .rename(columns={0:'var1'})\n",
    " .loc[:, df.columns]\n",
    ")\n",
    "\n",
    "print(reshaped)\n",
    "-------------------------------------------------------------------\n",
    "del df['C']\n",
    "df.drop(df.columns[[0, 2]], axis='columns')\n",
    "df.drop(['B', 'E'], axis='columns', inplace=True)\n",
    "\n",
    "------------------------------------------------------------------\n",
    "df.rename(columns={'old_name_1': 'new_name_1', 'old_name_2': 'new_name_2'}, inplace=True)\n",
    "df['C'] = df['A'] + df['B']\n",
    "\n",
    "-------------------------------------------------------------------\n",
    "re_order_column = ['B', 'C', 'A']\n",
    "df = df[re_column]\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "def countdown(n):\n",
    "    while n > 0:\n",
    "        yield n\n",
    "        n -= 1\n",
    "\n",
    "for i in countdown(5):\n",
    "    print(i)\n",
    "------------------------------------------------------------------\n",
    "import platform\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677bfb5-013d-454e-a258-bc47e39d1dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buff['EQP_ID'] = df_csv[\"6\"].str.split('_').str[0] \n",
    "df_buff['UNIT_NUM'] = df_csv[\"6\"].str.split('_').str[1] \n",
    "df_buff['COND_VALUES'] = df_csv[\"1\"].str.lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbc47c-85e6-4b73-81b9-39bf8125170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "__doc__  : Contains the docstring of the module, class, method, or function.\n",
    "__file__ : Contains the path to the script or module.\n",
    "__all__  : A list that defines what symbols are exported when from module import * is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d250882a-29b0-498d-a1b4-0274fe460e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory =  r\"C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\"\n",
    "file_pattern = '20*'  \n",
    "files = glob.glob(os.path.join(directory, file_pattern))\n",
    "files.sort()\n",
    "folder_path = files[-1]\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e4c855-412b-43cb-ac22-f0715f0b7529",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    num1 = float(input(\"Enter the numerator: \"))\n",
    "    num2 = float(input(\"Enter the denominator: \"))\n",
    "    result = divide_numbers(num1, num2)\n",
    "    if result is not None:\n",
    "        print(f\"Result: {result}\")\n",
    "except ValueError:\n",
    "    print(\"Error: Please enter valid numeric values.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "finally:\n",
    "    print(\"Execution completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb90463-4943-4e70-92bb-c6562b058abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd93960-490e-4d81-9bb4-44167eede6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Example 1: SyntaxError\n",
    "    print(\"Hello, world!\"\n",
    "\n",
    "    # Example 2: IndentationError\n",
    "    if True:\n",
    "    print(\"Indentation is incorrect.\")\n",
    "\n",
    "    # Example 3: TypeError\n",
    "    result = 10 + \"5\"\n",
    "\n",
    "    # Example 4: NameError\n",
    "    print(undefined_variable)\n",
    "\n",
    "    # Example 5: ValueError\n",
    "    number = int(\"abc\")\n",
    "\n",
    "    # Example 6: ZeroDivisionError\n",
    "    result = 10 / 0\n",
    "\n",
    "    # Example 7: FileNotFoundError\n",
    "    with open(\"nonexistent_file.txt\", \"r\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Example 8: IndexError\n",
    "    my_list = [1, 2, 3]\n",
    "    print(my_list[5])\n",
    "\n",
    "    # Example 9: KeyError\n",
    "    my_dict = {'a': 1, 'b': 2}\n",
    "    value = my_dict['c']\n",
    "\n",
    "    # Example 10: AttributeError\n",
    "    x = \"hello\"\n",
    "    length = x.length\n",
    "\n",
    "except SyntaxError as e:\n",
    "    print(f\"SyntaxError: {e}\")\n",
    "except IndentationError as e:\n",
    "    print(f\"IndentationError: {e}\")\n",
    "except TypeError as e:\n",
    "    print(f\"TypeError: {e}\")\n",
    "except NameError as e:\n",
    "    print(f\"NameError: {e}\")\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError: {e}\")\n",
    "except ZeroDivisionError as e:\n",
    "    print(f\"ZeroDivisionError: {e}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"FileNotFoundError: {e}\")\n",
    "except IndexError as e:\n",
    "    print(f\"IndexError: {e}\")\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}\")\n",
    "except AttributeError as e:\n",
    "    print(f\"AttributeError: {e}\")\n",
    "\n",
    "finally:\n",
    "    print(\"This will always be executed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a8618e-43fb-42e9-9a29-c7aac1a9e9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################SQL###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7678e81b-44fc-4974-940a-a696266f72a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logging.debug('Detailed information, typically of interest only when diagnosing problems')\n",
    "logging.info('Confirmation that things are working as expected.')\n",
    "logging.warning('Indication that something unexpected happened or an issue might arise soon The software is still functioning as expected.')\n",
    "logging.error('Indicates a more serious problem, the software is unable to perform some function.')\n",
    "logging.critical(' A very serious error, indicating that the program itself may be unable to continue running.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b36fbe-1d75-47f1-af33-da2002499cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"  SELECT 列名\n",
    "\"     FROM 表名          \n",
    "\"     WHERE 検索条件                  # 検索条件を指定します。\n",
    "\"     GROUP BY 列名                   # 行のグループの特性を調べる\n",
    "\"     HAVING 検索条件                 # GROUP BY 文節に基づいて選択されるグループに対し、検索条件を指定します\n",
    "\"     ORDER BY 列名                   # ユーザーが希望する順番を指定します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac34906-b17e-4c35-8db8-3628bd29f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat Table\n",
    "CREATE TABLE CONDUCTIVITY (\n",
    "    EQP_ID VARCHAR(8) NOT NULL,\n",
    "    UNIT_NUM VARCHAR(3) NOT NULL,\n",
    "    DATE_TIME TIMESTAMP NOT NULL,\n",
    "    VALUE DECIMAL(6,5),\n",
    "    PRIMARY KEY (EQP_ID, UNIT_NUM, DATE_TIME)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f800a8fa-2e01-4bd4-8379-d746cf1f360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect DB\n",
    "conn = ibm_db.connect(\"DATABASE=AYB_APPL;HOSTNAME=10.143.16.244;PORT=50000;PROTOCOL=TCPIP;UID=IOT_DATA;PWD=asd23fgh;\", \"\", \"\")\n",
    "if conn:\n",
    "    print(\"Connected to the database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d7341-2531-4863-8b07-cc969cc8ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "delete_query = \"DELETE FROM CONDUCTIVITY\"\n",
    "try:\n",
    "    ibm_db.autocommit(conn, ibm_db.SQL_AUTOCOMMIT_OFF)\n",
    "    if ibm_db.exec_immediate(conn, delete_query):\n",
    "        print(\"Existing data deleted successfully\")\n",
    "\n",
    "    ibm_db.commit(conn)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "    ibm_db.rollback(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04acc72e-946e-415f-a5f2-5760d9ec5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE with WHERE\n",
    "delete_query = \"DELETE FROM LIQUID_PARTICLE_LAST WHERE \tLOCATION = 'RP04'\"\n",
    "\n",
    "try:\n",
    "    ibm_db.autocommit(conn, ibm_db.SQL_AUTOCOMMIT_OFF)\n",
    "    if ibm_db.exec_immediate(conn, delete_query):\n",
    "        print(\"Selected rows deleted successfully\")\n",
    "\n",
    "    ibm_db.commit(conn)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "    ibm_db.rollback(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c029a07e-1add-464e-9994-f7f623dd75c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECT \n",
    "\n",
    "sql_query = \"SELECT * FROM CONDUCTIVITY\"\n",
    "stmt = ibm_db.exec_immediate(conn, sql_query)\n",
    "\n",
    "data = []\n",
    "row = ibm_db.fetch_assoc(stmt)\n",
    "\n",
    "while row:\n",
    "    data.append(row)\n",
    "    row = ibm_db.fetch_assoc(stmt)\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af318e-d172-4ece-9ebd-2a7d41b15c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE TWO TABLE\n",
    "MERGE INTO CONDUCTIVITY AS TGT\n",
    "USING (\n",
    "    VALUES \n",
    "        ('LEV50001', '020', TIMESTAMP '2023-12-04 15:01:09', 233),\n",
    "        ('LEV50001', '020', TIMESTAMP '2023-12-02 15:01:39', 4.00000),\n",
    "        ('LEV50001', '020', TIMESTAMP '2023-12-03 15:02:09', 5.00000)\n",
    ") AS INSERTDATA (EQP_ID, UNIT_NUM, DATE_TIME, COND_VALUES)\n",
    "ON (\n",
    "    TGT.EQP_ID = INSERTDATA.EQP_ID \n",
    "    AND TGT.UNIT_NUM = INSERTDATA.UNIT_NUM \n",
    "    AND TGT.DATE_TIME = INSERTDATA.DATE_TIME\n",
    ")\n",
    "WHEN MATCHED THEN \n",
    "    UPDATE SET TGT.COND_VALUES = INSERTDATA.COND_VALUES\n",
    "WHEN NOT MATCHED THEN \n",
    "    INSERT (EQP_ID, UNIT_NUM, DATE_TIME, COND_VALUES) \n",
    "    VALUES (INSERTDATA.EQP_ID, INSERTDATA.UNIT_NUM, INSERTDATA.DATE_TIME, INSERTDATA.COND_VALUES);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4cecbb-1ecc-47ae-b9fb-1cd8698cefd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. データがあればUPDATE, データがなければINSERTする。\n",
    "MERGE INTO LIQUID_PARTICLE_LAST AS target\n",
    "USING (SELECT ? AS LOCATION, ? AS TS_DT, ? AS VALUE_1_0, ? AS VALUE_3_0, ? AS VALUE_5_0, ? AS VALUE_10_0, ? AS VALUE_15_0, ? AS VALUE_20_0, ? AS VALUE_25_0, ? AS VALUE_50_0 FROM SYSIBM.SYSDUMMY1) AS source\n",
    "ON target.LOCATION = source.LOCATION\n",
    "WHEN MATCHED THEN\n",
    "    UPDATE SET target.TS_DT = source.TS_DT, \n",
    "               target.VALUE_1_0 = source.VALUE_1_0, \n",
    "               target.VALUE_3_0 = source.VALUE_3_0, \n",
    "               target.VALUE_5_0 = source.VALUE_5_0, \n",
    "               target.VALUE_10_0 = source.VALUE_10_0, \n",
    "               target.VALUE_15_0 = source.VALUE_15_0, \n",
    "               target.VALUE_20_0 = source.VALUE_20_0, \n",
    "               target.VALUE_25_0 = source.VALUE_25_0, \n",
    "               target.VALUE_50_0 = source.VALUE_50_0\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT (LOCATION, TS_DT, VALUE_1_0, VALUE_3_0, VALUE_5_0, VALUE_10_0, VALUE_15_0, VALUE_20_0, VALUE_25_0, VALUE_50_0)\n",
    "    VALUES (source.LOCATION, source.TS_DT, source.VALUE_1_0, source.VALUE_3_0, source.VALUE_5_0, source.VALUE_10_0, source.VALUE_15_0, source.VALUE_20_0, source.VALUE_25_0, source.VALUE_50_0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e0fd27-ad6b-4856-86ec-1a32209a1efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\\\Users\\\\00220401626\\\\Desktop\\\\FMS\\\\CsvData\\\\2023\\\\20231001.csv', encoding='cp932', dtype=object)\n",
    "df=df.iloc[:12000,::]\n",
    "\n",
    "df['Timestamp'] = df['Timestamp'].str.replace('/', '-', regex=False) \n",
    "\n",
    "data = tuple(tuple(row) for row in df.values )\n",
    "       \n",
    "b = \"\"\n",
    "for row in data:\n",
    "    b += str(row) +\",\"\n",
    "b = b[:-1]\n",
    "insert_query = f\"INSERT INTO LIQUID_PARTICLE_TEST (Location, TS_DT, VALUE_1_0, VALUE_3_0, VALUE_5_0, VALUE_10_0, VALUE_15_0, VALUE_20_0, VALUE_25_0, VALUE_50_0) VALUES {b}\"\n",
    "stmt = ibm_db.prepare(conn, insert_query)\n",
    "if ibm_db.execute(stmt,b):\n",
    "    print(\"Inserted successfully\")\n",
    "else:\n",
    "    print(\"Already updated\")\n",
    "    \n",
    "insertion_time = df['Timestamp'].max()\n",
    "json_data = {'insertion_time': insertion_time}\n",
    "with open('insertion_time.json', 'w') as json_file:\n",
    "    json.dump(json_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6079a75-b26d-41d7-804f-4ca5bf974894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. データがあればUPDATE, データがなければINSERTする\n",
    "\n",
    "merge_query = \"\"\"\n",
    "MERGE INTO LIQUID_PARTICLE_LAST AS target\n",
    "USING (SELECT ? AS LOCATION, ? AS TS_DT, ? AS VALUE_1_0, ? AS VALUE_3_0, ? AS VALUE_5_0, ? AS VALUE_10_0, ? AS VALUE_15_0, ? AS VALUE_20_0, ? AS VALUE_25_0, ? AS VALUE_50_0 FROM SYSIBM.SYSDUMMY1) AS source\n",
    "ON target.LOCATION = source.LOCATION\n",
    "WHEN MATCHED THEN\n",
    "    UPDATE SET target.TS_DT = source.TS_DT, \n",
    "               target.VALUE_1_0 = source.VALUE_1_0, \n",
    "               target.VALUE_3_0 = source.VALUE_3_0, \n",
    "               target.VALUE_5_0 = source.VALUE_5_0, \n",
    "               target.VALUE_10_0 = source.VALUE_10_0, \n",
    "               target.VALUE_15_0 = source.VALUE_15_0, \n",
    "               target.VALUE_20_0 = source.VALUE_20_0, \n",
    "               target.VALUE_25_0 = source.VALUE_25_0, \n",
    "               target.VALUE_50_0 = source.VALUE_50_0\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT (LOCATION, TS_DT, VALUE_1_0, VALUE_3_0, VALUE_5_0, VALUE_10_0, VALUE_15_0, VALUE_20_0, VALUE_25_0, VALUE_50_0)\n",
    "    VALUES (source.LOCATION, source.TS_DT, source.VALUE_1_0, source.VALUE_3_0, source.VALUE_5_0, source.VALUE_10_0, source.VALUE_15_0, source.VALUE_20_0, source.VALUE_25_0, source.VALUE_50_0);\n",
    "\"\"\"\n",
    "\n",
    "stmt = ibm_db.prepare(conn, merge_query)\n",
    "\n",
    "for index, row in latest_data.iterrows():\n",
    "    location = row['Location']\n",
    "    timestamp = row['Timestamp']\n",
    "    c1 = row['1.0μｍ']\n",
    "    c3 = row['3.0μｍ']\n",
    "    c5 = row['5.0μｍ']\n",
    "    c10 = row['10.0μｍ']\n",
    "    c15 = row['15.0μｍ']\n",
    "    c20 = row['20.0μｍ']\n",
    "    c25 = row['25.0μｍ']\n",
    "    c50 = row['50.0μｍ']\n",
    "    \n",
    "    if ibm_db.execute(stmt, (location, timestamp, c1, c3, c5, c10, c15, c20, c25, c50)):\n",
    "        print(f\"Row {index + 1} merged successfully\")\n",
    "    else:\n",
    "        print(f\"Error merging row {index + 1}: {ibm_db.stmt_errormsg()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac902b6-988c-4f01-acef-ce13c91a1fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT\n",
    "\n",
    "insert_query = \"INSERT INTO LIQUID_PARTICLE (LOCATION, TS_DT, VALUE_1_0, VALUE_3_0, VALUE_5_0, VALUE_10_0, VALUE_15_0, VALUE_20_0, VALUE_25_0, VALUE_50_0) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n",
    "\n",
    "insert = ibm_db.prepare(conn, insert_query)\n",
    "for index, row in df.iterrows():\n",
    "    location = row['Location']\n",
    "    timestamp = row['Timestamp']\n",
    "    c1 = row['1.0μｍ']\n",
    "    c3 = row['3.0μｍ']\n",
    "    c5 = row['5.0μｍ']\n",
    "    c10 = row['10.0μｍ']\n",
    "    c15 = row['15.0μｍ']\n",
    "    c20 = row['20.0μｍ']\n",
    "    c25 = row['25.0μｍ']\n",
    "    c50 = row['50.0μｍ']\n",
    "\n",
    "    if ibm_db.execute(insert, (location, timestamp, c1, c3, c5, c10, c15, c20, c25, c50)):\n",
    "        print(f\"Row {index + 1} inserted successfully\")\n",
    "    else:\n",
    "        print(f\"Error inserting row {index + 1}: {ibm_db.stmt_errormsg()}\")\n",
    "print('All Rows are inserted')\n",
    "ibm_db.commit(conn)\n",
    "ibm_db.close(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d8029c-708a-4be2-8582-515ad57f6fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE\n",
    "update_query = \"UPDATE your_table_name SET column1 = ? WHERE column2 = ?\"\n",
    "new_value = \"new_value\"\n",
    "where_value = \"some_condition\"\n",
    "params = (new_value, where_value)  \n",
    "stmt = ibm_db.prepare(conn, update_query)\n",
    "ibm_db.execute(stmt, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724436a7-8ece-4a92-8d23-fed86f060cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSON file\n",
    "data = {\n",
    "    \"database\": \"AYB_APPL\",\n",
    "    \"hostname\": \"10.143.16.244\",\n",
    "    \"port\": \"50000\",\n",
    "    \"protocol\": \"TCPIP\",\n",
    "    \"uid\": \"IOT_DATA\",\n",
    "    \"pwd\": \"asd23fgh\",\n",
    "    \"directory\":r\"C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\"\n",
    "}\n",
    "\n",
    "\n",
    "file_path = 'config.json'\n",
    "\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=2)\n",
    "\n",
    "print(f'JSON file \"{file_path}\" created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35261da-8ca2-4019-b87e-fac8c3c11921",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    driver   = 'SQL Server'\n",
    "    server   = '10.9.185.19'\n",
    "    database = 'treasure_test'\n",
    "    username = 'sa'\n",
    "    password = 'KyoceraAdmin'\n",
    "    \n",
    "    connection_string = f'DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password}'\n",
    "    connection = pyodbc.connect(connection_string)\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    csv_file_path = r\"C:\\Users\\00220401626\\Desktop\\ubexport_thememaster.csv\"\n",
    "    data = pd.read_csv(csv_file_path, encoding='cp932',dtype= object)\n",
    "\n",
    "\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        themeCD = row['テーマコード']\n",
    "        theme = row['テーマ名']\n",
    "        managerID = row['テーマリーダー']\n",
    "        manager = row['テーマリーダー（ユーザー名）']\n",
    "        product_line = row['プロダクトライン']\n",
    "        jigyobu = row['依頼元 事業部']\n",
    "        officeCD = row['テーマリーダー部署コード'][:3]\n",
    "        office = row[\"テーマリーダー部署コード\"]\n",
    "        divisionCD = str(row['テーマリーダー部署コード'][3:]) \n",
    "        division = row['テーマリーダー部署名'] \n",
    "        reg_date = str(pd.to_datetime(row['開始日'], format='%Y/%m/%d'))\n",
    "        comp_date = str(pd.to_datetime(row['完了予定日'], format='%Y/%m/%d'))\n",
    "        kouken_date_from = str(pd.to_datetime(row['税前貢献 開始年月'], format='%Y/%m'))\n",
    "        kouken_date_to = str(pd.to_datetime(row['税前貢献 終了年月'], format='%Y/%m'))\n",
    "        del_date =str(pd.to_datetime(row['抹消日'], format='%Y/%m/%d'))\n",
    "        kubun = row['テーマ区分']\n",
    "        devCD = row['開発コード']\n",
    "        memo = row['備考']\n",
    "        action_status = row['開発活動ステータス']\n",
    "        comp_result_date = str(pd.to_datetime(row['完了/中止/中断日'], format='%Y/%m/%d'))\n",
    "        four_quadrant = row['4象限区分']\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        sql= \"\"\"\n",
    "        MERGE  theme_master\n",
    "        USING (\n",
    "        SELECT ? AS themeCD, ? AS theme, ? AS managerID, ? AS manager, ? AS product_line,  ? AS jigyobu,  ? AS officeCD) as CSV\n",
    "    \n",
    "        ON (theme_master.themeCD = CSV.themeCD)\n",
    "        WHEN MATCHED THEN\n",
    "        \n",
    "        UPDATE SET\n",
    "        theme_master.theme = CSV.theme,\n",
    "        theme_master.managerID = CSV.managerID\n",
    " \n",
    "        \n",
    "          )\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(sql, (\n",
    "        themeCD, theme, managerID, manager, product_line, jigyobu, officeCD))\n",
    "        connection.commit()\n",
    "    \n",
    "        logging.info(\"Data import completed successfully.\")\n",
    "\n",
    "except pyodbc.Error as e:\n",
    "    logging.error(\"Error connecting to the database: %s\", e)\n",
    "\n",
    "finally:\n",
    "    cursor.close()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa15f56-78d4-4a8b-b5bb-509f254888b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"database\": \"APPL\",\n",
    "  \"host\": \"11.111.11.211\",\n",
    "  \"port\": \"50000\",\n",
    "  \"protocol\": \"TCPIP\",\n",
    "  \"uid\": \"DATA\",\n",
    "  \"pwd\": \"11111\",\n",
    "  \"pc_csv\": \"C:\\\\Users\\\\00220401626\\\\Desktop\\\\FMS\\\\CsvData\\\\2023\",\n",
    "  \"local_csv\": \"C:\\\\Users\\\\00220401626\\\\Desktop\\\\test\\\\CSV\",\n",
    "  \"log\": \"C:\\\\Users\\\\00220401626\\\\Desktop\\\\test\\\\debug.log\",\n",
    "  \"insertion_time\": \"2023-10-05 10:00:00\"\n",
    "}\n",
    "\n",
    "import os\n",
    "\n",
    "json_path = r'C:\\Users\\00220401626\\Desktop\\test\\config_test.json'\n",
    "\n",
    "with open(json_path, 'r') as input_file:\n",
    "    data = json.load(input_file)\n",
    "    \n",
    "data['insertion_time'] = \"2024-02-02 16:32:00\"\n",
    "json_data = json.dumps(data, indent=2)\n",
    "\n",
    "with open(json_path, 'w') as output_file:\n",
    "    output_file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b08a341-08fa-4799-8500-00fa8ed6d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "from os.path import join as path_join\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "json_path = r'C:\\Users\\00220401626\\Desktop\\test\\config.json'\n",
    "with open(json_path) as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "log = config['log']\n",
    "\n",
    "if not os.path.exists(log):\n",
    "    os.makedirs(log, exist_ok=True)\n",
    "\n",
    "dt_st = datetime.now()\n",
    "log_path = path_join(log, f\"debug_{dt_st.strftime('%Y%m%d')}.log\")\n",
    "\n",
    "log_formatter = logging.Formatter('%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "log_handler = RotatingFileHandler(filename=log_path, maxBytes=1048576, backupCount=10, delay=True)\n",
    "log_handler.setFormatter(log_formatter)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(log_handler)\n",
    "\n",
    "\n",
    "for i in range(100000):\n",
    "    logger.debug(f'This is a debug message {i}')\n",
    "    logger.info(f'This is an info message {i}')\n",
    "    logger.warning(f'This is a warning message {i}')\n",
    "    logger.error(f'This is an error message {i}')\n",
    "    logger.critical(f'This is a critical message {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b3548-cc63-48b0-8b81-6e3e2c51a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = glob.glob(os.path.join(local_csv,'*.csv'))\n",
    "    \n",
    "    dfs = []\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(fr\"{csv_file}\", encoding = 'cp932', dtype = object)\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    df['Timestamp'] = df['Timestamp'].str.replace('/', '-', regex=False)\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "    df = df[df['Timestamp'] > inserted_time]\n",
    "    data = tuple(tuple(row) for row in df.values )\n",
    "\n",
    "    values = \",\".join(map(str, data))\n",
    "    \n",
    "    insert_query = f\"INSERT INTO LIQUID_PARTICLE_TEST (Location, TS_DT, VALUE_1_0, VALUE_3_0, VALUE_5_0, VALUE_10_0, VALUE_15_0, VALUE_20_0, VALUE_25_0, VALUE_50_0) VALUES {values}\"\n",
    "    stmt = ibm_db.prepare(conn, insert_query)\n",
    "    \n",
    "    try:\n",
    "        if ibm_db.execute(stmt, values):\n",
    "             print(\"Data Inserted to Database\")       \n",
    "    except:\n",
    "        print(\"Already updated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884279ff-d821-4094-bb19-881e76efb806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE with WHERE\n",
    "delete_query = \"DELETE FROM LIQUID_PARTICLE_TEST \"\n",
    "# delete_query = \"DELETE FROM LIQUID_PARTICLE_LAST \"\n",
    "try:\n",
    "    ibm_db.autocommit(conn, ibm_db.SQL_AUTOCOMMIT_OFF)\n",
    "    if ibm_db.exec_immediate(conn, delete_query):\n",
    "        print(\"Selected rows deleted successfully\")\n",
    "\n",
    "    ibm_db.commit(conn)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "    ibm_db.rollback(conn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
