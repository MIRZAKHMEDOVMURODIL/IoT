{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495ee047-a33d-4b19-8035-7ea9ee4a5da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database\n",
      "Latest CSV file: C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231022.csv\n",
      "Row 1 merged successfully\n",
      "Row 2 merged successfully\n",
      "Row 3 merged successfully\n",
      "Row 4 merged successfully\n",
      "Row 5 merged successfully\n",
      "Row 6 merged successfully\n",
      "Row 7 merged successfully\n",
      "Row 8 merged successfully\n",
      "Row 9 merged successfully\n",
      "Row 10 merged successfully\n",
      "Row 11 merged successfully\n",
      "Row 12 merged successfully\n",
      "Row 13 merged successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231001.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231002.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231003.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231004.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231005.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231006.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231007.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231008.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231009.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231010.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231011.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231012.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231013.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231014.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231015.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231016.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231017.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231018.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231019.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231020.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231021.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231022.csv : Inserted successfully\n",
      "All CSV files are inserted\n",
      "Time spent: 53.136682987213135 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import ibm_db\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "import datetime as dt \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "conn = ibm_db.connect(\"DATABASE=AYB_APPL;HOSTNAME=10.143.16.244;PORT=50000;PROTOCOL=TCPIP;UID=IOT_DATA;PWD=asd23fgh;\", \"\", \"\")\n",
    "if conn:\n",
    "    print(\"Connected to the database\")\n",
    "\n",
    "\n",
    "    \n",
    "# 1) LOCATIONの最新のデータをINSERT/UPDATE\n",
    "\n",
    "\n",
    "#1.  最新フォルダの全CSVのPATHをまとめる。\n",
    "directory =  r\"C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\"\n",
    "file_pattern = '20*'  \n",
    "files = glob.glob(os.path.join(directory, file_pattern))\n",
    "files.sort()\n",
    "folder_path = files[-1]\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "\n",
    "\n",
    "#2. 最新のCSVファイルを探す。\n",
    "if csv_files:  \n",
    "    latest_csv_file = max(csv_files, key=os.path.getmtime)  \n",
    "    df = pd.read_csv(latest_csv_file, encoding='cp932',dtype= object)\n",
    " \n",
    "#3. CSVファイルのLocationカラムから最新の値を変数に入れる。\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    a = df['Location'].unique()\n",
    "    latest_data = pd.DataFrame()\n",
    "    df_concat = []\n",
    "    for location in a:\n",
    "        latest_location = df[df['Location'] == location].nlargest(1, 'Timestamp')\n",
    "        df_concat.append(latest_location)\n",
    "    latest_data = pd.concat(df_concat, ignore_index=True)\n",
    "    print(\"Latest CSV file:\", latest_csv_file)\n",
    "    \n",
    "else:\n",
    "    print(\"No CSV files found in the folder.\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# 4. データがあればUPDATE, データがなければINSERTする。\n",
    "merge_query = \"\"\"\n",
    "MERGE INTO LIQUID_PARTICLE_LAST AS target\n",
    "USING (SELECT ? AS LOCATION, ? AS TS_DT, ? AS VALUE_1_0, ? AS VALUE_3_0, ? AS VALUE_5_0, ? AS VALUE_10_0, ? AS VALUE_15_0, ? AS VALUE_20_0, ? AS VALUE_25_0, ? AS VALUE_50_0 FROM SYSIBM.SYSDUMMY1) AS source\n",
    "ON target.LOCATION = source.LOCATION\n",
    "WHEN MATCHED THEN\n",
    "    UPDATE SET target.TS_DT = source.TS_DT, \n",
    "               target.VALUE_1_0 = source.VALUE_1_0, \n",
    "               target.VALUE_3_0 = source.VALUE_3_0, \n",
    "               target.VALUE_5_0 = source.VALUE_5_0, \n",
    "               target.VALUE_10_0 = source.VALUE_10_0, \n",
    "               target.VALUE_15_0 = source.VALUE_15_0, \n",
    "               target.VALUE_20_0 = source.VALUE_20_0, \n",
    "               target.VALUE_25_0 = source.VALUE_25_0, \n",
    "               target.VALUE_50_0 = source.VALUE_50_0\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT (LOCATION, TS_DT, VALUE_1_0, VALUE_3_0, VALUE_5_0, VALUE_10_0, VALUE_15_0, VALUE_20_0, VALUE_25_0, VALUE_50_0)\n",
    "    VALUES (source.LOCATION, source.TS_DT, source.VALUE_1_0, source.VALUE_3_0, source.VALUE_5_0, source.VALUE_10_0, source.VALUE_15_0, source.VALUE_20_0, source.VALUE_25_0, source.VALUE_50_0);\n",
    "\"\"\"\n",
    "\n",
    "stmt = ibm_db.prepare(conn, merge_query)\n",
    "\n",
    "for index, row in latest_data.iterrows():\n",
    "    location = row['Location']\n",
    "    timestamp = row['Timestamp']\n",
    "    c1 = row['1.0μｍ']\n",
    "    c3 = row['3.0μｍ']\n",
    "    c5 = row['5.0μｍ']\n",
    "    c10 = row['10.0μｍ']\n",
    "    c15 = row['15.0μｍ']\n",
    "    c20 = row['20.0μｍ']\n",
    "    c25 = row['25.0μｍ']\n",
    "    c50 = row['50.0μｍ']\n",
    "    \n",
    "    if ibm_db.execute(stmt, (location, timestamp, c1, c3, c5, c10, c15, c20, c25, c50)):\n",
    "        print(f\"Row {index + 1} merged successfully\")\n",
    "    else:\n",
    "        print(f\"Error merging row {index + 1}: {ibm_db.stmt_errormsg()}\")\n",
    "\n",
    "\n",
    "        \n",
    "#############################################################################################################\n",
    "        \n",
    "# 2) JSON記録に記載された日より、以降のデータをCSVファイルごとにINSERTする\n",
    "\n",
    "        \n",
    "#　1.JSONの記録からデータベースに最後INSERTされた時刻を取得する\n",
    "\n",
    "with open('insertion_time.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    inserted_time = dt.datetime.strptime(data['insertion_time'], '%Y-%m-%d %H:%M:%S')\n",
    "    inserted_day = inserted_time.strftime('%Y%m%d')\n",
    "    time_to_compare = data['insertion_time']\n",
    "\n",
    "    \n",
    "\n",
    "#  2.最新のフォルダから,全CSVのPATHをまとめる\n",
    "\n",
    "directory =  r\"C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\"\n",
    "file_pattern = '20*'  \n",
    "files = glob.glob(os.path.join(directory, file_pattern))\n",
    "files.sort()\n",
    "folder_path = files[-1]\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 　3.JSON記録に記載された日と、それより以降のCSV1をまとめる  \n",
    "selected_csv_files=[]\n",
    "selected_csv_files = [file for file in csv_files if int(file[-12:-4]) >= int(inserted_day)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#   4.JSON記録に記載された日より以降のCSV1を１つずつINSERTする  \n",
    "if selected_csv_files:\n",
    "    \n",
    "    for i in selected_csv_files:\n",
    "        df = pd.read_csv(fr\"{i}\", encoding='cp932', dtype= object)\n",
    "        df['Timestamp'] = df['Timestamp'].str.replace('/', '-', regex=False)\n",
    "        \n",
    "        a = df[df['Timestamp'] >  time_to_compare]\n",
    "        data = tuple(tuple(row) for row in a.values )\n",
    "           \n",
    "        b = \"\"\n",
    "        for row in data:\n",
    "            b += str(row) +\",\"\n",
    "        b = b[:-1]\n",
    "        \n",
    "        insert_query = f\"INSERT INTO LIQUID_PARTICLE_TEST (Location, TS_DT, VALUE_1_0, VALUE_3_0, VALUE_5_0, VALUE_10_0, VALUE_15_0, VALUE_20_0, VALUE_25_0, VALUE_50_0) VALUES {b}\"\n",
    "        stmt = ibm_db.prepare(conn, insert_query)\n",
    "        \n",
    "        try:\n",
    "            if ibm_db.execute(stmt, b):\n",
    "                print(f\"{i} : Inserted successfully\")     \n",
    "        except:\n",
    "            print(f\"({i}) is already updated\")\n",
    "                \n",
    "        \n",
    "#   5.時間を JSON ファイルに保存する   \n",
    "    \n",
    "        insertion_time = a['Timestamp'].max()\n",
    "        json_data = {'insertion_time': insertion_time}\n",
    "        with open('insertion_time.json', 'w') as json_file:\n",
    "            json.dump(json_data, json_file, indent=4)\n",
    "    print('All CSV files are inserted')\n",
    "elif not selected_csv_files:\n",
    "    print(\"Database is already updated\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time spent: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff83101-84f8-48a2-babe-bffc750c755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e74237f-1662-4c7c-88d1-6614b49897b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(json_file_path):\n",
    "#     with open(json_file_path, 'r') as json_file:\n",
    "#         data = json.load(json_file)\n",
    "#         inserted_time = dt.datetime.strptime(data['insertion_time'], '%Y-%m-%d %H:%M:%S')\n",
    "#         inserted_day = inserted_time.strftime('%Y%m%d')\n",
    "#         time_to_compare = data['insertion_time']\n",
    "#         print(f\"Last Insertion time: {time_to_compare}\")\n",
    "#     directory =  r\"C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\"\n",
    "#     file_pattern = '20*'  \n",
    "#     files = glob.glob(os.path.join(directory, file_pattern))\n",
    "#     files.sort()\n",
    "#     folder_path = files[-1]\n",
    "#     csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "#     selected_csv_files=[]\n",
    "#     selected_csv_files = [file for file in csv_files if int(file[-12:-4]) >= int(inserted_day)]\n",
    "#     selected_csv_files\n",
    "# else:\n",
    "#     print(\"JSON file does not exist. All CSV files below are going to be inserted into the database.\")\n",
    "#     logging.info(\"JSON file does not exist. All CSV files below are going to be inserted into the database.\")\n",
    "#     directory =  r\"C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\"\n",
    "#     file_pattern = '20*'  \n",
    "#     files = glob.glob(os.path.join(directory, file_pattern))\n",
    "#     files.sort()\n",
    "#     folder_path = files[-1]\n",
    "#     selected_csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2277ca0-e9fd-46c3-a342-2a0321c6f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import ibm_db\n",
    "import logging\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "import datetime as dt \n",
    "import shutil\n",
    "\n",
    "\n",
    "json_path = r'C:\\Users\\00220401626\\Desktop\\test\\config.json'\n",
    "with open(json_path) as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "database = config['database']\n",
    "host = config['host']\n",
    "port = config['port']\n",
    "protocol = config['protocol']\n",
    "uid = config['uid']\n",
    "pwd = config['pwd']\n",
    "pc_csv = config['pc_csv']\n",
    "local_csv = config['local_csv']\n",
    "log = config['log']\n",
    "insertion_time = config['insertion_time']\n",
    "\n",
    "time = str(insertion_time)\n",
    "inserted_time = dt.datetime.strptime(time, '%Y-%m-%d %H:%M:%S')\n",
    "inserted_day = inserted_time.strftime('%Y%m%d')\n",
    "\n",
    "print(inserted_time)\n",
    "print(inserted_day)\n",
    "\n",
    "logging.basicConfig(filename=log, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def copy_local():\n",
    "    if insertion_time:    \n",
    "        csv_files = glob.glob(os.path.join(pc_csv,'*.csv'))\n",
    "        selected_csv_paths = [file for file in csv_files if int(file[-12:-4]) >= int(inserted_day)]\n",
    "    else:\n",
    "        selected_csv_paths = glob.glob(os.path.join(pc_csv,'*.csv'))\n",
    "        \n",
    "    for source_path in selected_csv_paths:\n",
    "        if os.path.exists(source_path):  \n",
    "            file_name = os.path.basename(source_path)\n",
    "            destination_path = os.path.join(local_csv, file_name)\n",
    "            shutil.copy(source_path, destination_path)\n",
    "            print(f\"'{source_path}' copied.\")\n",
    "        else:\n",
    "            print(f\"Source file '{source_path}' does not exist.\")\n",
    "    \n",
    "def connect_db():\n",
    "    dsn = f\"DATABASE={database};HOSTNAME={host};PORT={port};PROTOCOL={protocol};UID={uid};PWD={pwd}\"\n",
    "    conn = ibm_db.connect(dsn, \"\", \"\")\n",
    "    if conn:\n",
    "        logging.info(\"Connected to the database\")\n",
    "        print(\"Connected to the database\")\n",
    "    else:\n",
    "        logging.info(\"Failed to connect to the database\") \n",
    "        print(\"Not connected to the database\")\n",
    "    return conn\n",
    "\n",
    "def latest_data(local_csv):\n",
    "    try:\n",
    "        csv_files = glob.glob(os.path.join(local_csv, '*.csv'))\n",
    "        if csv_files:\n",
    "            latest_csv_file = max(csv_files, key=os.path.getmtime)                                                                #max(int(csv_files[-12:-4]))\n",
    "            df = pd.read_csv(f\"{latest_csv_file}\", encoding='cp932', dtype=object)\n",
    "        \n",
    "        if not df.empty:\n",
    "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "            unique = df['Location'].unique()\n",
    "            \n",
    "            df_concat = []\n",
    "            for location in unique:\n",
    "                latest_location = df[df['Location'] == location].nlargest(1, 'Timestamp')\n",
    "                df_concat.append(latest_location)\n",
    "                \n",
    "            latest_data = pd.concat(df_concat, ignore_index=True)\n",
    "            print(latest_data)\n",
    "            merge_query = \"\"\"\n",
    "                MERGE INTO LIQUID_PARTICLE_LAST AS target\n",
    "                USING (SELECT ? AS LOCATION, ? AS TS_DT, ? AS VALUE_1_0, ? AS VALUE_3_0, ? AS VALUE_5_0, ? AS VALUE_10_0, ? AS VALUE_15_0, ? AS VALUE_20_0, ? AS VALUE_25_0, ? AS VALUE_50_0 FROM SYSIBM.SYSDUMMY1) AS source\n",
    "                ON target.LOCATION = source.LOCATION\n",
    "                WHEN MATCHED THEN\n",
    "                    UPDATE SET target.TS_DT = source.TS_DT, \n",
    "                               target.VALUE_1_0 = source.VALUE_1_0, \n",
    "                               target.VALUE_3_0 = source.VALUE_3_0, \n",
    "                               target.VALUE_5_0 = source.VALUE_5_0, \n",
    "                               target.VALUE_10_0 = source.VALUE_10_0, \n",
    "                               target.VALUE_15_0 = source.VALUE_15_0, \n",
    "                               target.VALUE_20_0 = source.VALUE_20_0, \n",
    "                               target.VALUE_25_0 = source.VALUE_25_0, \n",
    "                               target.VALUE_50_0 = source.VALUE_50_0\n",
    "                WHEN NOT MATCHED THEN\n",
    "                    INSERT (LOCATION, TS_DT, VALUE_1_0, VALUE_3_0, VALUE_5_0, VALUE_10_0, VALUE_15_0, VALUE_20_0, VALUE_25_0, VALUE_50_0)\n",
    "                    VALUES (source.LOCATION, source.TS_DT, source.VALUE_1_0, source.VALUE_3_0, source.VALUE_5_0, source.VALUE_10_0, source.VALUE_15_0, source.VALUE_20_0, source.VALUE_25_0, source.VALUE_50_0);\n",
    "                \"\"\"\n",
    "        \n",
    "            stmt = ibm_db.prepare(conn, merge_query)\n",
    "        \n",
    "            for index, row in latest_data.iterrows():\n",
    "                location = row['Location']\n",
    "                timestamp = row['Timestamp']\n",
    "                c1 = row['1.0μｍ']\n",
    "                c3 = row['3.0μｍ']\n",
    "                c5 = row['5.0μｍ']\n",
    "                c10 = row['10.0μｍ']\n",
    "                c15 = row['15.0μｍ']\n",
    "                c20 = row['20.0μｍ']\n",
    "                c25 = row['25.0μｍ']\n",
    "                c50 = row['50.0μｍ']\n",
    "        \n",
    "                try:\n",
    "                    if ibm_db.execute(stmt, (location, timestamp, c1, c3, c5, c10, c15, c20, c25, c50)):\n",
    "                        logging.info(f\"Row {index + 1} merged successfully\")\n",
    "                    else:\n",
    "                        error_message = f\"Error merging row {index + 1}: {ibm_db.stmt_errormsg()}\"\n",
    "                        logging.error(error_message)\n",
    "                except Exception as e:\n",
    "                    error_message = f\"Exception merging row {index + 1}: {str(e)}\"\n",
    "                    logging.error(error_message)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    \n",
    "def all_data(local_csv):\n",
    "    csv_files = glob.glob(os.path.join(local_csv, '*.csv'))\n",
    "    if csv_files:  \n",
    "        for csv in csv_files:\n",
    "            df = pd.read_csv(fr\"{csv}\", encoding='cp932', dtype= object)\n",
    "            df['Timestamp'] = df['Timestamp'].str.replace('/', '-', regex=False)\n",
    "          \n",
    "            config['insertion_time'] = str(df['Timestamp'].max())\n",
    "            json_data = json.dumps(config, indent=2)\n",
    "            \n",
    "            print(json_data)\n",
    "            \n",
    "            df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%Y-%m-%d %H:%M:%S') \n",
    "            \n",
    "            if os.path.exists(json_path) and inserted_time:\n",
    "                df = df[df['Timestamp'] > inserted_time]\n",
    "                data = tuple(tuple(row) for row in df.values)\n",
    "                values = \",\".join(map(str, data))\n",
    "            \n",
    "                insert_query = f\"INSERT INTO LIQUID_PARTICLE_TEST (Location, TS_DT, VALUE_1_0, VALUE_3_0, VALUE_5_0, VALUE_10_0, VALUE_15_0, VALUE_20_0, VALUE_25_0, VALUE_50_0) VALUES {values}\"\n",
    "                stmt = ibm_db.prepare(conn, insert_query)\n",
    "            \n",
    "                try:\n",
    "                    if ibm_db.execute(stmt, values):\n",
    "                        logging.info(f\"{csv} : Inserted successfully\")\n",
    "                        print(f\"{csv} : Inserted successfully\")\n",
    "                            \n",
    "                        with open(json_path, 'w') as output_file:\n",
    "                            output_file.write(json_data)\n",
    "                except:\n",
    "                    logging.info(f\"{csv} : is already updated\")\n",
    "                    print(f\"{csv} : is already updated\")\n",
    "              \n",
    "            else:\n",
    "                data = tuple(tuple(row) for row in df.values)\n",
    "                values = \",\".join(map(str, data))\n",
    "            \n",
    "                insert_query = f\"INSERT INTO LIQUID_PARTICLE_TEST (Location, TS_DT, VALUE_1_0, VALUE_3_0, VALUE_5_0, VALUE_10_0, VALUE_15_0, VALUE_20_0, VALUE_25_0, VALUE_50_0) VALUES {values}\"\n",
    "                stmt = ibm_db.prepare(conn, insert_query)\n",
    "            \n",
    "                try:\n",
    "                    if ibm_db.execute(stmt, values):\n",
    "                        logging.info(f\"{csv} : Inserted successfully\")\n",
    "                        \n",
    "                        with open(json_path, 'w') as output_file:\n",
    "                            output_file.write(json_data)\n",
    "                except:\n",
    "                    logging.info(f\"{csv} : is already updated\")\n",
    "                    print(f\"{csv} : is already updated\")\n",
    "         \n",
    "        logging.info('All CSV files are inserted')\n",
    "    \n",
    "    elif not selected_csv_files:\n",
    "        logging.info(\"Database is already updated\")\n",
    "\n",
    "\n",
    "def backup_and_delete_csv_files(local_csv, nas):\n",
    "    csv_files = glob.glob(os.path.join(local_csv,'*.csv'))\n",
    "    for source_path in csv_files:\n",
    "        if os.path.exists(source_path):  \n",
    "            file_name = os.path.basename(source_path)\n",
    "            destination_path = os.path.join(nas, file_name)\n",
    "            shutil.copy(source_path, destination_path)\n",
    "            os.remove(source_path)\n",
    "            print(f\"File '{file_name}' copied to NAS and deleted from local_csv.\")\n",
    "        else:\n",
    "            print(f\"Source file '{source_path}' does not exist.\")\n",
    "\n",
    "# start_time = time.time()   \n",
    "\n",
    "copy_local()\n",
    "conn = connect_db()\n",
    "# latest_data(local_csv)\n",
    "all_data(local_csv)\n",
    "# backup_and_delete_csv_files(local_csv, nas)\n",
    "\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# minutes = int(elapsed_time // 60)\n",
    "# seconds = int(elapsed_time % 60)\n",
    "# logging.info(f\"Time spent: {minutes} minutes and {seconds} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
