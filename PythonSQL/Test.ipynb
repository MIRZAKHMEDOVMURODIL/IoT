{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c24bc5d-fc50-4f2d-9995-bbbfbbfb8325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database\n",
      "\n",
      "Function_1: Proccess started\n",
      "Latest CSV file: C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2023\\20231022.csv\n",
      "Row 1 merged successfully\n",
      "Row 2 merged successfully\n",
      "Row 3 merged successfully\n",
      "Row 4 merged successfully\n",
      "Row 5 merged successfully\n",
      "Row 6 merged successfully\n",
      "Row 7 merged successfully\n",
      "Row 8 merged successfully\n",
      "Row 9 merged successfully\n",
      "Row 10 merged successfully\n",
      "Row 11 merged successfully\n",
      "Row 12 merged successfully\n",
      "Row 13 merged successfully\n",
      "Time spent: 0 minutes and 0 seconds\n",
      "Function_1: Completed\n",
      "\n",
      "Function_2: Proccess started\n",
      "Last Insertion time: 2023-10-19 23:59:55\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2023\\20231019.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2023\\20231020.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2023\\20231021.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2023\\20231022.csv\n",
      "CSV files are inserting.......\n",
      "(C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2023\\20231019.csv) is already updated\n",
      "(C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2023\\20231020.csv) is already updated\n",
      "(C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2023\\20231021.csv) is already updated\n",
      "(C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2023\\20231022.csv) is already updated\n",
      "All CSV files are inserted\n",
      "Time spent: 1 minutes and 20 seconds\n",
      "Function_2: Completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import ibm_db\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "import datetime as dt \n",
    "import numpy as np\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "\n",
    "# ハンドラを設定\n",
    "handler = RotatingFileHandler(filename='logfile.log', maxBytes=1000000, backupCount=5)\n",
    "\n",
    "# ログメッセージのフォーマットを指定\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "\n",
    "# ロギングの基本設定を行い、ハンドラを追加\n",
    "logging.basicConfig(level=logging.INFO, handlers=[handler])\n",
    "\n",
    "#########################################################################################################################################################################\n",
    "\n",
    "\n",
    "try:  \n",
    "    # 'config.json' ファイルを開いて、設定データを読み込む\n",
    "    with open('config.json') as config_file:\n",
    "        config_data = json.load(config_file)\n",
    "\n",
    "    # 設定データから必要な情報を取得\n",
    "    database = config_data.get(\"database\")\n",
    "    hostname = config_data.get(\"hostname\")\n",
    "    port = config_data.get(\"port\")\n",
    "    protocol = config_data.get(\"protocol\")\n",
    "    uid = config_data.get(\"uid\")\n",
    "    pwd = config_data.get(\"pwd\")\n",
    "    global directory\n",
    "    directory = config_data.get(\"directory\")\n",
    "    \n",
    "    # 接続文字列を構築  \n",
    "    conn_str = f\"DATABASE={database};HOSTNAME={hostname};PORT={port};PROTOCOL={protocol};UID={uid};PWD={pwd};\"\n",
    "\n",
    "    # IBM Db2 データベースに接続\n",
    "    conn = ibm_db.connect(conn_str, \"\", \"\")\n",
    "\n",
    "    # 接続が成功したかどうかを確認\n",
    "    if conn:\n",
    "        print(\"Connected to the database\\n\")\n",
    "        logging.info(\"Connected to the database\\n\")\n",
    "    else:\n",
    "        print(\"Failed to connect to the database\\n\")\n",
    "        logging.error(\"Failed to connect to the database\\n\")\n",
    "   \n",
    "except Exception as e:\n",
    "    logging.error(f\"An unknown error occurred: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################################################################\n",
    "\n",
    "# 1) 各LOCATIONごとに最新のデータをINSERT/UPDATE  \n",
    "\n",
    "def function_1():\n",
    "    \n",
    "    print(\"Function_1: Proccess started\") \n",
    "    logging.info(\"Function_1: Proccess started\")\n",
    "          \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #1.最新フォルダ内のすべてのCSVファイルのパスを収集する。\n",
    "        file_pattern = '20*'\n",
    "        files = glob.glob(os.path.join(directory, file_pattern))\n",
    "        files.sort()\n",
    "        \n",
    "        \n",
    "        #2. 最新のCSVファイルを探す。\n",
    "        if files:\n",
    "            # 最新のフォルダのパスを取得し、その中のCSVファイルを抽出\n",
    "            folder_path = files[-1]\n",
    "            csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "        \n",
    "            if csv_files:\n",
    "                 # 最新のCSVファイルを取得し、Pandasを使用してデータフレームに読み込む\n",
    "                latest_csv_file = max(csv_files, key=os.path.getmtime)\n",
    "                df = pd.read_csv(latest_csv_file, encoding='cp932', dtype=object)\n",
    "                \n",
    "                \n",
    "        #3. Locationカラムから最新の値を変数に入れる。\n",
    "                \n",
    "                # 日時型に変換\n",
    "                df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "                \n",
    "                # ユニークなロケーションを取得\n",
    "                unique_locations = df['Location'].unique()\n",
    "                \n",
    "                # 最新のデータを格納するためのデータフレームを作成\n",
    "                latest_data = pd.DataFrame()\n",
    "                df_concat = []\n",
    "                \n",
    "                # ユニークなロケーションごとに最新のデータを選択し、データフレームに追加\n",
    "                for location in unique_locations:\n",
    "                    latest_location = df[df['Location'] == location].nlargest(1, 'Timestamp')\n",
    "                    df_concat.append(latest_location)\n",
    "                    \n",
    "                # 各ロケーションの最新データを結合   \n",
    "                latest_data = pd.concat(df_concat, ignore_index=True)\n",
    "\n",
    "                # ログとコンソールに処理結果を出力\n",
    "                print(\"Latest CSV file:\", latest_csv_file)\n",
    "                logging.info(\"Latest CSV file: %s\", latest_csv_file)\n",
    "                logging.info(\"Processed data for %d locations\", len(unique_locations))\n",
    "                \n",
    "            else:\n",
    "                print(\"No CSV files found in the latest folder.\")\n",
    "                logging.warning(\"No CSV files found in the latest folder.\")\n",
    "        else:\n",
    "            print(\"No folders found.\")\n",
    "            logging.warning(\"No folders found.\")\n",
    "           \n",
    "         \n",
    "        # 4. データがあればUPDATE, データがなければINSERTする。\n",
    "        merge_query = \"\"\"\n",
    "        MERGE INTO LIQUID_PARTICLE_LAST AS target\n",
    "        USING (SELECT ? AS LOCATION, ? AS TS_DT, ? AS VALUE_1_0, ? AS VALUE_3_0, ? AS VALUE_5_0, ? AS VALUE_10_0, ? AS VALUE_15_0, ? AS VALUE_20_0, ? AS VALUE_25_0, ? AS VALUE_50_0 FROM SYSIBM.SYSDUMMY1) AS source\n",
    "        ON target.LOCATION = source.LOCATION\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET target.TS_DT = source.TS_DT, \n",
    "                       target.VALUE_1_0 = source.VALUE_1_0, \n",
    "                       target.VALUE_3_0 = source.VALUE_3_0, \n",
    "                       target.VALUE_5_0 = source.VALUE_5_0, \n",
    "                       target.VALUE_10_0 = source.VALUE_10_0, \n",
    "                       target.VALUE_15_0 = source.VALUE_15_0, \n",
    "                       target.VALUE_20_0 = source.VALUE_20_0, \n",
    "                       target.VALUE_25_0 = source.VALUE_25_0, \n",
    "                       target.VALUE_50_0 = source.VALUE_50_0\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT (LOCATION, TS_DT, VALUE_1_0, VALUE_3_0, VALUE_5_0, VALUE_10_0, VALUE_15_0, VALUE_20_0, VALUE_25_0, VALUE_50_0)\n",
    "            VALUES (source.LOCATION, source.TS_DT, source.VALUE_1_0, source.VALUE_3_0, source.VALUE_5_0, source.VALUE_10_0, source.VALUE_15_0, source.VALUE_20_0, source.VALUE_25_0, source.VALUE_50_0);\n",
    "        \"\"\"\n",
    "        \n",
    "        stmt = ibm_db.prepare(conn, merge_query)\n",
    "\n",
    "        # 最新データをループしてマージ\n",
    "        for index, row in latest_data.iterrows():\n",
    "            # 行から必要なデータを抽出\n",
    "            location = row['Location']\n",
    "            timestamp = row['Timestamp']\n",
    "            c1 = row['1.0μｍ']\n",
    "            c3 = row['3.0μｍ']\n",
    "            c5 = row['5.0μｍ']\n",
    "            c10 = row['10.0μｍ']\n",
    "            c15 = row['15.0μｍ']\n",
    "            c20 = row['20.0μｍ']\n",
    "            c25 = row['25.0μｍ']\n",
    "            c50 = row['50.0μｍ']\n",
    "\n",
    "            # データをマージする\n",
    "            if ibm_db.execute(stmt, (location, timestamp, c1, c3, c5, c10, c15, c20, c25, c50)):\n",
    "                print(f\"Row {index + 1} merged successfully\")\n",
    "                logging.info(f\"Row {index + 1} merged successfully\")\n",
    "            else:\n",
    "                print(f\"Error merging row {index + 1}: {ibm_db.stmt_errormsg()}\")\n",
    "\n",
    "\n",
    "\n",
    "        # 処理時間の計測と表示\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        minutes = int(elapsed_time // 60)\n",
    "        seconds = int(elapsed_time % 60)\n",
    "        print(f\"Time spent: {minutes} minutes and {seconds} seconds\")\n",
    "        logging.info(f\"Time spent: {minutes} minutes and {seconds} seconds\")\n",
    "    except Exception as e:   \n",
    "        logging.error(f\"An unknown error occurred: {str(e)}\")\n",
    "\n",
    "    print(\"Function_1: Completed\\n\") \n",
    "    logging.info(\"Function_1: Completed\\n\") \n",
    "#####################################################################################################################################\n",
    "        \n",
    "# 2) JSON記録に記載された日より以降のデータをCSVファイルごとにINSERTする\n",
    "\n",
    "        \n",
    "    #1.　JSON 記録から最後INSERTされた行の時間を取得します。\n",
    "    #2.　最新のフォルダから,全てのCSVファイルのパスを取得します。\n",
    "    #3.　JSONに記載された最後の行以降のデータをまとめる。  \n",
    "\n",
    "\n",
    "def function_2():\n",
    "    print(\"Function_2: Proccess started\") \n",
    "    logging.info(\"Function_2: Proccess started\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        json_file_path = 'insertion_time.json'\n",
    "        \n",
    "        # JSONファイルが存在する場合   \n",
    "        if os.path.exists(json_file_path):\n",
    "            with open(json_file_path, 'r') as json_file:\n",
    "                data = json.load(json_file)\n",
    "                if 'insertion_time' in data and data['insertion_time'] and not pd.isna(data['insertion_time'])  :\n",
    "                    \n",
    "                    # 'insertion_time'が存在し、nanでない場合\n",
    "                    insertion_time_raw = str(data['insertion_time'])\n",
    "                    inserted_time = dt.datetime.strptime(insertion_time_raw, '%Y-%m-%d %H:%M:%S')\n",
    "                    inserted_day = inserted_time.strftime('%Y%m%d')\n",
    "                    time_to_compare = insertion_time_raw\n",
    "                    print(f\"Last Insertion time: {time_to_compare}\")\n",
    "                    \n",
    "                    # CSVファイルの検索と選択\n",
    "                    # directory = r\"C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\"\n",
    "                    file_pattern = '20*'\n",
    "                    files = glob.glob(os.path.join(directory, file_pattern))\n",
    "                    files.sort()\n",
    "                    folder_path = files[-1]\n",
    "                    csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "                    selected_csv_files = [file for file in csv_files if int(file[-12:-4]) >= int(inserted_day)]\n",
    "                \n",
    "                else:\n",
    "                     # 'insertion_time'がnanまたは見つからない場合\n",
    "                    print(\"Error: 'insertion_time' is 'nan' or not found in the JSON file.\\nAll CSV files below are going to be inserted into the database.\")\n",
    "                    logging.error(\"Error: 'insertion_time' is 'nan' or not found in the JSON file. \\nAll CSV files below are going to be inserted into the database.\")\n",
    "                    # directory = r\"C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\"\n",
    "                    file_pattern = '20*'\n",
    "                    files = glob.glob(os.path.join(directory, file_pattern))\n",
    "                    files.sort()\n",
    "                    folder_path = files[-1]\n",
    "                    selected_csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "                    time_to_compare = False\n",
    "             \n",
    "        else:\n",
    "            # JSONファイルが存在しない場合\n",
    "            print(\"JSON file does not exist. \\nAll CSV files below are going to be inserted into the database.\")\n",
    "            logging.info(\"JSON file does not exist. All CSV files below are going to be inserted into the database.\")\n",
    "            # directory = r\"C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\"\n",
    "            file_pattern = '20*'\n",
    "            files = glob.glob(os.path.join(directory, file_pattern))\n",
    "            files.sort()\n",
    "            folder_path = files[-1]\n",
    "            selected_csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "            time_to_compare = False\n",
    "            \n",
    "        # 選択されたCSVファイルの表示とログ出力\n",
    "        for i in selected_csv_files:\n",
    "            print(i)\n",
    "            logging.info(i)\n",
    "            \n",
    "        print(\"CSV files are inserting.......\")\n",
    "        logging.info(\"CSV files are inserting.......\")\n",
    "         \n",
    "\n",
    "        \n",
    "        \n",
    "    # 4.JSONに記載された最後の行より以降のデータのcsvを１つずつINSERTする\n",
    "        if selected_csv_files:\n",
    " \n",
    "            # 選択されたCSVファイルごとに処理\n",
    "            for i in selected_csv_files:\n",
    "                 # CSVファイルをデータフレームとして読み込み  \n",
    "         \n",
    "               \n",
    "                df = pd.read_csv(fr\"{i}\", encoding='cp932', dtype= object)\n",
    "                \n",
    "                # タイムスタンプの書式を整える\n",
    "                df['Timestamp'] = df['Timestamp'].str.replace('/', '-', regex=False)\n",
    "\n",
    "                # JSONファイルが存在し、挿入時刻が指定されている場合\n",
    "                if os.path.exists(json_file_path) and time_to_compare:\n",
    "\n",
    "                    # 指定時刻以降のデータを抽出\n",
    "                    a = df[df['Timestamp'] >  time_to_compare]\n",
    "                    data = tuple(tuple(row) for row in a.values )\n",
    "\n",
    "                    # データを文字列に変換してINSERTクエリを構築\n",
    "                    b = \"\"\n",
    "                    for row in data:\n",
    "                        b += str(row) +\",\"\n",
    "                    b = b[:-1]\n",
    "                    \n",
    "                    insert_query = f\"INSERT INTO LIQUID_PARTICLE (Location, TS_DT, VALUE_1_0, VALUE_3_0, VALUE_5_0, VALUE_10_0, VALUE_15_0, VALUE_20_0, VALUE_25_0, VALUE_50_0) VALUES {b}\"\n",
    "                    stmt = ibm_db.prepare(conn, insert_query)\n",
    "                    \n",
    "                    try:\n",
    "                        # データをデータベースに挿入\n",
    "                        if ibm_db.execute(stmt, b):\n",
    "                            print(f\"{i} : Inserted successfully\") \n",
    "                            logging.info(f\"{i} : Inserted successfully\")\n",
    "                    except:\n",
    "                        print(f\"({i}) is already updated\")\n",
    "                        logging.info(f\"{i} : is already updated\")\n",
    "\n",
    "\n",
    "                        \n",
    "                    #5.時間を JSON ファイルに保存する   \n",
    "                    insertion_time = a['Timestamp'].max()\n",
    "                    if not pd.isna(insertion_time):\n",
    "                        json_data = {'insertion_time': insertion_time}\n",
    "                        with open('insertion_time.json', 'w') as json_file:\n",
    "                            json.dump(json_data, json_file, indent=4)\n",
    "                            \n",
    "                else:\n",
    "                    \n",
    "                    data = tuple(tuple(row) for row in df.values )\n",
    "\n",
    "                    # データを文字列に変換してINSERTクエリを構築\n",
    "                    b = \"\"\n",
    "                    for row in data:\n",
    "                        b += str(row) +\",\"\n",
    "                    b = b[:-1]\n",
    "                    \n",
    "                    insert_query = f\"INSERT INTO LIQUID_PARTICLE (Location, TS_DT, VALUE_1_0, VALUE_3_0, VALUE_5_0, VALUE_10_0, VALUE_15_0, VALUE_20_0, VALUE_25_0, VALUE_50_0) VALUES {b}\"\n",
    "                    stmt = ibm_db.prepare(conn, insert_query)\n",
    "\n",
    "                    try:\n",
    "                         # データをデータベースに挿入\n",
    "                        if ibm_db.execute(stmt, b):\n",
    "                            print(f\"{i} : Inserted successfully\") \n",
    "                            logging.info(f\"{i} : Inserted successfully\")\n",
    "                    except:\n",
    "                        print(f\"({i}) is already updated\")\n",
    "                        logging.info(f\"{i} : is already updated\")\n",
    "                        \n",
    "                \n",
    "                    #5.時間を JSON ファイルに保存する          \n",
    "                \n",
    "                    insertion_time = df['Timestamp'].max()\n",
    "                    if not pd.isna(insertion_time):\n",
    "                        json_data = {'insertion_time': insertion_time}\n",
    "                        with open('insertion_time.json', 'w') as json_file:\n",
    "                            json.dump(json_data, json_file, indent=4)\n",
    "    \n",
    "                        \n",
    "            print('All CSV files are inserted')\n",
    "            logging.info('All CSV files are inserted')\n",
    "            \n",
    "        elif not selected_csv_files:\n",
    "            print(\"Database is already updated\")\n",
    "            logging.info(\"Database is already updated\")\n",
    "            \n",
    "        # 処理時間の計測と表示\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        minutes = int(elapsed_time // 60)\n",
    "        seconds = int(elapsed_time % 60)\n",
    "        print(f\"Time spent: {minutes} minutes and {seconds} seconds\")\n",
    "        logging.info(f\"Time spent: {minutes} minutes and {seconds} seconds\")\n",
    "        \n",
    "    except Exception as e:   \n",
    "        logging.error(f\"An unknown error occurred: {str(e)}\")\n",
    "  \n",
    "    print(\"Function_2: Completed\\n\") \n",
    "    logging.info(\"Function_2: Completed\\n\")\n",
    "          \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # データベースに接続できている場合\n",
    "    if conn:\n",
    "        function_1()           # Function_1の実行\n",
    "        function_2()           # Function_2の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56c898-ab1c-4489-ac29-bc53f9699125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
