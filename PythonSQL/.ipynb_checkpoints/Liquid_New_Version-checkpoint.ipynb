{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a022f21d-5852-4e62-b0fb-854115959216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import ibm_db\n",
    "import logging\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import datetime as dt \n",
    "import shutil\n",
    "from os.path import join\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "\n",
    "# Load Configuration from JSON file\n",
    "json_path = r'C:\\Work\\20_particle\\config.json'\n",
    "with open(json_path) as json_file:\n",
    "    config = json.load(json_file)\n",
    "    \n",
    "# Extract Configuration values\n",
    "database = config['database']\n",
    "host = config['host']\n",
    "port = config['port']\n",
    "protocol = config['protocol']\n",
    "uid = config['uid']\n",
    "pwd = config['pwd']\n",
    "pc_csv = config['pc_csv']\n",
    "local_csv = config['local_csv']\n",
    "log = config['log']\n",
    "insertion_time = config['insertion_time']\n",
    "nas = config['nas']\n",
    "time = str(insertion_time)\n",
    "inserted_time = dt.datetime.strptime(time, '%Y-%m-%d %H:%M:%S')\n",
    "inserted_day = inserted_time.strftime('%Y%m%d')\n",
    "\n",
    "# Logging Settings\n",
    "if not os.path.exists(log):\n",
    "    os.makedirs(log, exist_ok=True)\n",
    "\n",
    "dt_st = datetime.now()\n",
    "log_path = join(log, f\"debug_{dt_st.strftime('%Y%m%d')}.log\")\n",
    "log_formatter = logging.Formatter('%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "log_handler = RotatingFileHandler(filename=log_path, maxBytes=1048576, backupCount=10, delay=True)\n",
    "log_handler.setFormatter(log_formatter)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(log_handler)\n",
    "\n",
    "logging.info(\"1.Extract JSON Configuration Values\")\n",
    "print(\"1.Extract JSON Configuration Values\")\n",
    "\n",
    "# Copy CSV files from PC to Temp_CSV folder \n",
    "def copy_local():\n",
    "    logging.info(\"2.Copy CSV Files to TEMP_CVS Folder\")\n",
    "    print(\"2.Copy CSV Files to TEMP_CVS Folder\")\n",
    "    \n",
    "    if insertion_time:    \n",
    "        csv_files = glob.glob(os.path.join(pc_csv,'*.csv'))\n",
    "        selected_csv_paths = [file for file in csv_files if int(file[-12:-4]) >= int(inserted_day)]\n",
    "    else:\n",
    "        selected_csv_paths = glob.glob(os.path.join(pc_csv,'*.csv'))\n",
    "        \n",
    "    for source_path in selected_csv_paths:\n",
    "        if os.path.exists(source_path):  \n",
    "            file_name = os.path.basename(source_path)\n",
    "            destination_path = os.path.join(local_csv, file_name)\n",
    "            \n",
    "            try:\n",
    "                shutil.copy(source_path, destination_path)\n",
    "                logging.info(f\"'{file_name}' is copied to TEMP_CSV folder.\")\n",
    "                print(f\"'{file_name}' is copied to TEMP_CSV folder.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error copying '{file_name}': {str(e)}\")\n",
    "                print(f\"Error copying '{file_name}' : {str(e)}\")\n",
    "        else:\n",
    "            logging.warning(f\"Source file '{source_path}' does not exist.\")\n",
    "            print(f\"Source file '{source_path}' does not exist.\")\n",
    "\n",
    "# Database Connection\n",
    "def connect_db():\n",
    "    logging.info(\"3.Database Connection\")\n",
    "    print(\"3.Database Connection\")\n",
    "    dsn = f\"DATABASE={database};HOSTNAME={host};PORT={port};PROTOCOL={protocol};UID={uid};PWD={pwd}\"\n",
    "    conn = ibm_db.connect(dsn, \"\", \"\")\n",
    "    if conn:\n",
    "        logging.info(\"Connected to the database\")\n",
    "        print(\"Connected to the database\")\n",
    "    else:\n",
    "        logging.info(\"Failed to connect to the database\") \n",
    "        print(\"Not connected to the database\")\n",
    "    return conn\n",
    "\n",
    "#Inserting the Latest Values into the Database\n",
    "def latest_data(local_csv):\n",
    "    logging.info(\"4.Latest_Values Insertion\") \n",
    "    print(\"4.Latest_Values Insertion\")\n",
    "    try:\n",
    "        csv_files = glob.glob(os.path.join(local_csv, '*.csv'))\n",
    "        if csv_files:\n",
    "            latest_csv_file = max(csv_files, key=os.path.getmtime)                                                                #max(int(csv_files[-12:-4]))\n",
    "            df = pd.read_csv(f\"{latest_csv_file}\", encoding='cp932', dtype=object)\n",
    "        \n",
    "        if not df.empty:\n",
    "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "            unique = df['Location'].unique()\n",
    "            \n",
    "            df_concat = []\n",
    "            for location in unique:\n",
    "                latest_location = df[df['Location'] == location].nlargest(1, 'Timestamp')\n",
    "                df_concat.append(latest_location)\n",
    "                \n",
    "            latest_data = pd.concat(df_concat, ignore_index=True).sort_values(by='Location')\n",
    "            merge_query = \"\"\"\n",
    "                MERGE INTO LIQUID_PARTICLE_LAST AS target\n",
    "                USING (SELECT ? AS LOCATION, ? AS TS_DT, ? AS VALUE_1_0, ? AS VALUE_3_0, ? AS VALUE_5_0, ? AS VALUE_10_0, ? AS VALUE_15_0, ? AS VALUE_20_0, ? AS VALUE_25_0, ? AS VALUE_50_0 FROM SYSIBM.SYSDUMMY1) AS source\n",
    "                ON target.LOCATION = source.LOCATION\n",
    "                WHEN MATCHED THEN\n",
    "                    UPDATE SET target.TS_DT = source.TS_DT, \n",
    "                               target.VALUE_1_0 = source.VALUE_1_0, \n",
    "                               target.VALUE_3_0 = source.VALUE_3_0, \n",
    "                               target.VALUE_5_0 = source.VALUE_5_0, \n",
    "                               target.VALUE_10_0 = source.VALUE_10_0, \n",
    "                               target.VALUE_15_0 = source.VALUE_15_0, \n",
    "                               target.VALUE_20_0 = source.VALUE_20_0, \n",
    "                               target.VALUE_25_0 = source.VALUE_25_0, \n",
    "                               target.VALUE_50_0 = source.VALUE_50_0\n",
    "                WHEN NOT MATCHED THEN\n",
    "                    INSERT (LOCATION, TS_DT, VALUE_1_0, VALUE_3_0, VALUE_5_0, VALUE_10_0, VALUE_15_0, VALUE_20_0, VALUE_25_0, VALUE_50_0)\n",
    "                    VALUES (source.LOCATION, source.TS_DT, source.VALUE_1_0, source.VALUE_3_0, source.VALUE_5_0, source.VALUE_10_0, source.VALUE_15_0, source.VALUE_20_0, source.VALUE_25_0, source.VALUE_50_0);\n",
    "                \"\"\"\n",
    "        \n",
    "            stmt = ibm_db.prepare(conn, merge_query)\n",
    "        \n",
    "            for index, row in latest_data.iterrows():\n",
    "                location = row['Location']\n",
    "                timestamp = row['Timestamp']\n",
    "                c1 = row['1.0μｍ']\n",
    "                c3 = row['3.0μｍ']\n",
    "                c5 = row['5.0μｍ']\n",
    "                c10 = row['10.0μｍ']\n",
    "                c15 = row['15.0μｍ']\n",
    "                c20 = row['20.0μｍ']\n",
    "                c25 = row['25.0μｍ']\n",
    "                c50 = row['50.0μｍ']\n",
    "        \n",
    "                try:\n",
    "                    if ibm_db.execute(stmt, (location, timestamp, c1, c3, c5, c10, c15, c20, c25, c50)):\n",
    "                        logging.info(f\"{location} : Inserted to Database\")\n",
    "                        print(f\"{location} : Inserted to Database\")\n",
    "                    else:\n",
    "                        error_message = f\"Error {location}: {ibm_db.stmt_errormsg()}\"\n",
    "                        logging.error(error_message)\n",
    "                except Exception as e:\n",
    "                    error_message = f\"Exception {location}: {str(e)}\"\n",
    "                    logging.error(error_message)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "#Inserting the All Values into the Database   \n",
    "def all_data(local_csv):\n",
    "    logging.info(\"5.All_Values Insertion\") \n",
    "    print(\"5.All_Values Insertion\")\n",
    "    \n",
    "    csv_files = glob.glob(os.path.join(local_csv, '*.csv'))\n",
    "    if csv_files:  \n",
    "        for csv in csv_files:\n",
    "            df = pd.read_csv(fr\"{csv}\", encoding='cp932', dtype= object)\n",
    "            df['Timestamp'] = df['Timestamp'].str.replace('/', '-', regex=False)\n",
    "          \n",
    "            config['insertion_time'] = str(df['Timestamp'].max())\n",
    "            json_data = json.dumps(config, indent=2)\n",
    "                        \n",
    "            df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%Y-%m-%d %H:%M:%S') \n",
    "            \n",
    "            if os.path.exists(json_path) and inserted_time:\n",
    "                df = df[df['Timestamp'] > inserted_time]\n",
    "                data = tuple(tuple(row) for row in df.values)\n",
    "                values = \",\".join(map(str, data))\n",
    "            \n",
    "                insert_query = f\"INSERT INTO LIQUID_PARTICLE_TEST (Location, TS_DT, VALUE_1_0, VALUE_3_0, VALUE_5_0, VALUE_10_0, VALUE_15_0, VALUE_20_0, VALUE_25_0, VALUE_50_0) VALUES {values}\"\n",
    "                stmt = ibm_db.prepare(conn, insert_query)\n",
    "            \n",
    "                try:\n",
    "                    if ibm_db.execute(stmt, values):\n",
    "                        logging.info(f\"{csv} : Inserted to Database\")\n",
    "                        print(f\"{csv} : Inserted to Database\")   \n",
    "                        with open(json_path, 'w') as output_file:\n",
    "                            output_file.write(json_data)\n",
    "                except:\n",
    "                    logging.info(f\"{csv} : is already Updated\")\n",
    "                    print(f\"{csv} : is already Updated\")\n",
    "              \n",
    "            else:\n",
    "                data = tuple(tuple(row) for row in df.values)\n",
    "                values = \",\".join(map(str, data))\n",
    "            \n",
    "                insert_query = f\"INSERT INTO LIQUID_PARTICLE_TEST (Location, TS_DT, VALUE_1_0, VALUE_3_0, VALUE_5_0, VALUE_10_0, VALUE_15_0, VALUE_20_0, VALUE_25_0, VALUE_50_0) VALUES {values}\"\n",
    "                stmt = ibm_db.prepare(conn, insert_query)\n",
    "            \n",
    "                try:\n",
    "                    if ibm_db.execute(stmt, values):\n",
    "                        logging.info(f\"{csv} : Inserted to Database\")\n",
    "                        print(f\"{csv} : Inserted to Database\")\n",
    "                        with open(json_path, 'w') as output_file:\n",
    "                            output_file.write(json_data)\n",
    "                except:\n",
    "                    logging.info(f\"{csv} : is already Updated\")\n",
    "                    print(f\"{csv} : is already Updated\")\n",
    "\n",
    "    \n",
    "    else:\n",
    "        logging.info(\"Database is already Updated\")\n",
    "\n",
    "# Backup CSV files to NAS and Delete from TEMP_CSV folder\n",
    "def backup_and_delete_csv_files(local_csv, nas):\n",
    "    logging.info(\"6.Backup_and_Delete_CSV_Files\") \n",
    "    print(\"6.Backup_and_Delete_CSV_Files\")\n",
    "    \n",
    "    csv_files = glob.glob(os.path.join(local_csv,'*.csv'))\n",
    "    for source_path in csv_files:\n",
    "        if os.path.exists(source_path):  \n",
    "            file_name = os.path.basename(source_path)\n",
    "            folder_name = file_name[:6]  \n",
    "            \n",
    "            destination_folder = os.path.join(nas, folder_name)\n",
    "            \n",
    "            destination_path = os.path.join(destination_folder, file_name)\n",
    "            if not os.path.exists(destination_folder): \n",
    "                os.makedirs(destination_folder)\n",
    "                print(f\"Created folder '{folder_name}' in NAS\")\n",
    "                logging.info(f\"Created folder '{folder_name}' in NAS\")\n",
    "\n",
    "            shutil.copy(source_path, destination_path)\n",
    "            os.remove(source_path)\n",
    "            print(f\"File '{file_name}' copied to NAS and Deleted from TEMP_CSV\")\n",
    "            logging.info(f\"File '{file_name}' copied to NAS and Deleted from TEMP_CSV\")\n",
    "        else:\n",
    "            print(f\"Source file '{source_path}' does not exist.\")\n",
    "            logging.info(f\"Source file '{source_path}' does not exist.\")\n",
    "\n",
    "# Delete Log files if created day period passed 30 days                          \n",
    "def delete_old_logs(log, days=30):\n",
    "    period_time = datetime.now() - timedelta(days=days)\n",
    "\n",
    "    for filename in os.listdir(log):\n",
    "        if filename.endswith(\".log\"):\n",
    "            filepath = os.path.join(log, filename)\n",
    "            try:\n",
    "                if os.path.isfile(filepath) and datetime.fromtimestamp(os.path.getctime(filepath)) < period_time:\n",
    "                    os.remove(filepath)\n",
    "                    logging.info(f\"Deleted log file: {filepath}\")\n",
    "                    print(f\"Deleted log file: {filepath}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting file {filepath}: {e}\")\n",
    "                logging.info(f\"Error deleting file {filepath}: {e}\")\n",
    "\n",
    "# Running ALl Functions in Order\n",
    "start_time = datetime.now()   \n",
    "\n",
    "copy_local()\n",
    "conn = connect_db()\n",
    "latest_data(local_csv)\n",
    "all_data(local_csv)\n",
    "backup_and_delete_csv_files(local_csv, nas)\n",
    "delete_old_logs(log)\n",
    "\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "total_seconds = elapsed_time.total_seconds()\n",
    "minutes = int(total_seconds // 60)\n",
    "seconds = int(total_seconds % 60)\n",
    "\n",
    "logging.info(f\"Time spent: {minutes} minutes and {seconds} seconds\")\n",
    "print(f\"Time spent: {minutes} minutes and {seconds} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
