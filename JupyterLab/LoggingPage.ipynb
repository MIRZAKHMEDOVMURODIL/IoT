{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea01b878-825b-4905-84be-245577e39325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import ibm_db\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "import datetime as dt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbb7dc6c-7140-46c5-92bb-f020b4a322c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import ibm_db\n",
    "\n",
    "json_path = r'C:\\Users\\00220401626\\Desktop\\test\\config.json'\n",
    "log_path = r'C:\\Users\\00220401626\\Desktop\\test\\debug.log'\n",
    "logging.basicConfig(filename=log_file_path, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def connect_to_database(config_file_path, log_file_path):\n",
    "    \n",
    "    with open(config_file_path) as json_file:\n",
    "        config = json.load(json_file)\n",
    "\n",
    "    dsn = f\"DATABASE={config['database']};HOSTNAME={config['host']};PORT={config['port']};PROTOCOL={config['protocol']};UID={config['uid']};PWD={config['pwd']}\"\n",
    "    conn = ibm_db.connect(dsn, \"\", \"\")\n",
    "\n",
    "    if conn:\n",
    "        logging.info(\"Connected to the database\")\n",
    "        print(\"Connected to the database\")\n",
    "    else:\n",
    "        logging.error(\"Failed to connect to the database\")\n",
    "        print(\"Failed to connect to the database\")\n",
    "        exit()\n",
    "\n",
    "connect_to_database(json_path, log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d53c9a33-0136-4828-bb41-e20b14080f9a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'database': 'AYB_APPL',\n",
       " 'host': '10.143.16.244',\n",
       " 'port': '50000',\n",
       " 'protocol': 'TCPIP',\n",
       " 'uid': 'IOT_DATA',\n",
       " 'pwd': 'asd23fgh',\n",
       " 'directory': 'C:\\\\10.143.64.59\\\\ParticleData'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "log_path = r\"C:\\Work\\20_particle\\debug.log\"\n",
    "\n",
    "try:\n",
    "    logging.basicConfig(filename=log_path, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    df = pd.read_csv(r\"\\\\10.143.64.59\\ParticleData\\20240213.csv\", encoding='cp932', dtype=object)\n",
    "    logging.info(\"Test: %s\", df)\n",
    "except Exception as e:\n",
    "    logging.error(\"An error occurred: %s\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a995e04-e7ba-4975-af5b-5b8cabbf5831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "logging.basicConfig(filename=log_path, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "directory = r\"C:\\Users\\00220401626\\Desktop\\Ayabe\\FMS\\CsvData\"\n",
    "log_path = r\"C:\\Users\\00220401626\\Desktop\\debug.log\"\n",
    "\n",
    "files = glob.glob(os.path.join(directory, '*'))\n",
    "files.sort()\n",
    "\n",
    "if not files:\n",
    "    logging.info(\"No files found in the folder.\")\n",
    "else:\n",
    "    folder_path = files[-1]\n",
    "    csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "       \n",
    "    if csv_files:\n",
    "        latest_csv_file = max(csv_files, key=os.path.getmtime)\n",
    "        df = pd.read_csv(latest_csv_file, encoding='cp932', dtype=object)\n",
    "\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "        a = df['Location'].unique()\n",
    "        latest_data = pd.DataFrame()\n",
    "        df_concat = []\n",
    "\n",
    "        for location in a:\n",
    "            latest_location = df[df['Location'] == location].nlargest(1, 'Timestamp')\n",
    "            df_concat.append(latest_location)\n",
    "\n",
    "        latest_data = pd.concat(df_concat, ignore_index=True)\n",
    "        logging.info(\"Latest CSV file: %s\", latest_csv_file)\n",
    "        logging.info(\"Latest CSV file: %s\", latest_data)\n",
    "\n",
    "    else:\n",
    "        logging.info(\"No CSV files found in the folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "127c35b6-c845-4a07-ba7f-34fbd1193993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>1.0μｍ</th>\n",
       "      <th>3.0μｍ</th>\n",
       "      <th>5.0μｍ</th>\n",
       "      <th>10.0μｍ</th>\n",
       "      <th>15.0μｍ</th>\n",
       "      <th>20.0μｍ</th>\n",
       "      <th>25.0μｍ</th>\n",
       "      <th>50.0μｍ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RP11</td>\n",
       "      <td>2023-10-02 23:59:00</td>\n",
       "      <td>7405</td>\n",
       "      <td>234</td>\n",
       "      <td>216</td>\n",
       "      <td>174</td>\n",
       "      <td>150</td>\n",
       "      <td>132</td>\n",
       "      <td>121</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RP09</td>\n",
       "      <td>2023-10-02 23:59:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RP10</td>\n",
       "      <td>2023-10-02 23:59:00</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RP12</td>\n",
       "      <td>2023-10-02 23:59:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RP08</td>\n",
       "      <td>2023-10-02 23:59:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RP01</td>\n",
       "      <td>2023-10-02 23:58:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RP07</td>\n",
       "      <td>2023-10-02 23:59:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RP04</td>\n",
       "      <td>2023-10-02 23:59:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RP02</td>\n",
       "      <td>2023-10-02 23:59:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RP05</td>\n",
       "      <td>2023-10-02 23:59:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RP03</td>\n",
       "      <td>2023-10-02 23:59:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RP06</td>\n",
       "      <td>2023-10-02 23:59:00</td>\n",
       "      <td>9644</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RP13</td>\n",
       "      <td>2023-10-02 23:59:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Location           Timestamp 1.0μｍ 3.0μｍ 5.0μｍ 10.0μｍ 15.0μｍ 20.0μｍ 25.0μｍ  \\\n",
       "0      RP11 2023-10-02 23:59:00  7405   234   216    174    150    132    121   \n",
       "1      RP09 2023-10-02 23:59:00     0     0     0      0      0      0      0   \n",
       "2      RP10 2023-10-02 23:59:00    16     2     2      2      2      2      2   \n",
       "3      RP12 2023-10-02 23:59:00     0     0     0      0      0      0      0   \n",
       "4      RP08 2023-10-02 23:59:00     0     0     0      0      0      0      0   \n",
       "5      RP01 2023-10-02 23:58:00     0     0     0      0      0      0      0   \n",
       "6      RP07 2023-10-02 23:59:00     0     0     0      0      0      0      0   \n",
       "7      RP04 2023-10-02 23:59:00     4     0     0      0      0      0      0   \n",
       "8      RP02 2023-10-02 23:59:00     0     0     0      0      0      0      0   \n",
       "9      RP05 2023-10-02 23:59:00     0     0     0      0      0      0      0   \n",
       "10     RP03 2023-10-02 23:59:00     0     0     0      0      0      0      0   \n",
       "11     RP06 2023-10-02 23:59:00  9644    15     0      0      0      0      0   \n",
       "12     RP13 2023-10-02 23:59:00     0     0     0      0      0      0      0   \n",
       "\n",
       "   50.0μｍ  \n",
       "0      73  \n",
       "1       0  \n",
       "2       2  \n",
       "3       0  \n",
       "4       0  \n",
       "5       0  \n",
       "6       0  \n",
       "7       0  \n",
       "8       0  \n",
       "9       0  \n",
       "10      0  \n",
       "11      0  \n",
       "12      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path = r\"C:\\Users\\00220401626\\Desktop\\test\\debug.log\"\n",
    "logging.basicConfig(filename=log_path, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "directory = r\"C:\\Users\\00220401626\\Desktop\\Ayabe\\FMS\\CsvData\\2021\"\n",
    "csv_files = glob.glob(os.path.join(directory, '*.csv'))\n",
    "logging.info(\"csv_files path: %s\", csv_files)       \n",
    "if csv_files:\n",
    "    latest_csv_file = max(csv_files, key=os.path.getmtime)\n",
    "    df = pd.read_csv(latest_csv_file, encoding='cp932', dtype=object)\n",
    "\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    a = df['Location'].unique()\n",
    "    latest_data = pd.DataFrame()\n",
    "    df_concat = []\n",
    "\n",
    "    for location in a:\n",
    "        latest_location = df[df['Location'] == location].nlargest(1, 'Timestamp')\n",
    "        df_concat.append(latest_location)\n",
    "\n",
    "    latest_data = pd.concat(df_concat, ignore_index=True)\n",
    "    logging.info(\"Latest CSV file: %s\", latest_csv_file)\n",
    "    logging.info(\"Latest CSV file: %s\", latest_data)\n",
    "\n",
    "else:\n",
    "    logging.info(\"No CSV files found in the folder.\")\n",
    "latest_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1d8f6-b7d6-4c43-91bd-545fd3ce42a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if csv_files:\n",
    "        latest_csv_file = max(csv_files, key=os.path.getmtime)\n",
    "        df = pd.read_csv(latest_csv_file, encoding='cp932', dtype=object)\n",
    "\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "        unique_locations = df['Location'].unique()\n",
    "        latest_data = pd.DataFrame()\n",
    "        df_concat = []\n",
    "\n",
    "        for location in unique_locations:\n",
    "            latest_location = df[df['Location'] == location].nlargest(1, 'Timestamp')\n",
    "            df_concat.append(latest_location)\n",
    "\n",
    "        latest_data = pd.concat(df_concat, ignore_index=True)\n",
    "        logging.info(\"Latest CSV file: %s\", latest_csv_file)\n",
    "        logging.info(\"Latest data:\\n%s\", latest_data)\n",
    "\n",
    "    else:\n",
    "        logging.info(\"No CSV files found in the folder.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(\"An error occurred: %s\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ec6c326-14ff-4290-95f1-e5b9a964273c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\logging\\__init__.py\", line 1098, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\logging\\__init__.py\", line 942, in format\n",
      "    return fmt.format(record)\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\logging\\__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\logging\\__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Temp\\ipykernel_18788\\2196685814.py\", line 26, in <cell line: 11>\n",
      "    logging.info(\"Latest CSV file:\", latest_csv_file)\n",
      "Message: 'Latest CSV file:'\n",
      "Arguments: ('C:\\\\Users\\\\00220401626\\\\Desktop\\\\Ayabe\\\\FMS\\\\CsvData\\\\2023\\\\20231022.csv',)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\logging\\__init__.py\", line 1098, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\logging\\__init__.py\", line 942, in format\n",
      "    return fmt.format(record)\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\logging\\__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\logging\\__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\00220401626\\AppData\\Local\\Temp\\ipykernel_18788\\2196685814.py\", line 27, in <cell line: 11>\n",
      "    logging.info(\"Latest CSV file:\", latest_data)\n",
      "Message: 'Latest CSV file:'\n",
      "Arguments: (   Location           Timestamp 1.0μｍ 3.0μｍ 5.0μｍ 10.0μｍ 15.0μｍ 20.0μｍ 25.0μｍ  \\\n",
      "0      RP07 2023-10-22 23:59:53     0     0     0      0      0      0      0   \n",
      "1      RP09 2023-10-22 23:59:53     0     0     0      0      0      0      0   \n",
      "2      RP10 2023-10-22 23:59:52    19     2     0      0      0      0      0   \n",
      "3      RP04 2023-10-22 23:59:33     1     0     0      0      0      0      0   \n",
      "4      RP08 2023-10-22 23:59:53     0     0     0      0      0      0      0   \n",
      "5      RP13 2023-10-22 23:59:55     0     0     0      0      0      0      0   \n",
      "6      RP01 2023-10-22 23:58:31     0     0     0      0      0      0      0   \n",
      "7      RP06 2023-10-22 23:59:52     0     0     0      0      0      0      0   \n",
      "8      RP03 2023-10-22 23:59:33     0     0     0      0      0      0      0   \n",
      "9      RP05 2023-10-22 23:59:33     0     0     0      0      0      0      0   \n",
      "10     RP11 2023-10-22 23:59:53     0     0     0      0      0      0      0   \n",
      "11     RP02 2023-10-22 23:59:32     0     0     0      0      0      0      0   \n",
      "12     RP12 2023-10-22 23:59:14     0     0     0      0      0      0      0   \n",
      "\n",
      "   50.0μｍ  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n",
      "5       0  \n",
      "6       0  \n",
      "7       0  \n",
      "8       0  \n",
      "9       0  \n",
      "10      0  \n",
      "11      0  \n",
      "12      0  ,)\n"
     ]
    }
   ],
   "source": [
    "# directory = r\"C:\\10.143.64.59\"\n",
    "directory = r\"C:\\Users\\00220401626\\Desktop\\Ayabe\\FMS\\CsvData\"\n",
    "files = glob.glob(os.path.join(directory, '*'))\n",
    "files.sort()\n",
    "folder_path = files[-1]\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "log_path = r\"C:\\Users\\00220401626\\Desktop\\debug.log\"\n",
    "logging.basicConfig(filename=log_path, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "if csv_files:\n",
    "  \n",
    "    latest_csv_file = max(csv_files, key=os.path.getmtime)\n",
    "    df = pd.read_csv(latest_csv_file, encoding='cp932', dtype=object)\n",
    "\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    a = df['Location'].unique()\n",
    "    latest_data = pd.DataFrame()\n",
    "    df_concat = []\n",
    "\n",
    "    for location in a:\n",
    "        latest_location = df[df['Location'] == location].nlargest(1, 'Timestamp')\n",
    "        df_concat.append(latest_location)\n",
    "\n",
    "    latest_data = pd.concat(df_concat, ignore_index=True)\n",
    "    logging.info(\"Latest CSV file:\", latest_csv_file)\n",
    "    logging.info(\"Latest CSV file:\", latest_data)\n",
    "\n",
    "else:\n",
    "    logging.info(\"No CSV files found in the folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84baf246-75f5-441d-bbc2-efc9285b62f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file \"config.json\" created successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = {\n",
    "    \"database\": \"AYB_APPL\",\n",
    "    \"host\": \"10.143.16.244\",\n",
    "    \"port\": \"50000\",\n",
    "    \"protocol\":\"TCPIP\",\n",
    "    \"uid\":\"IOT_DATA\",\n",
    "    \"pwd\":\"asd23fgh\",\n",
    "    \"log_path\" : r'C:\\Users\\00220401626\\Desktop\\test\\debug.log',\n",
    "    \"csv_path\" :  r'C:\\Users\\00220401626\\Desktop\\Ayabe\\FMS\\CsvData\\2023'\n",
    "}\n",
    "\n",
    "file_path = r'C:\\Users\\00220401626\\Desktop\\test\\config.json'\n",
    "\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=2)\n",
    "\n",
    "print(f'JSON file \"{file_path}\" created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d47f660c-9ea5-44d9-8df3-0aaa4a265250",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     i \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcp932\u001b[39m\u001b[38;5;124m\"\u001b[39m, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[1;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:347\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[0;32m    145\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    155\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    along the other axes.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:404\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    401\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "csv_folder=r\"C:\\Users\\00220401626\\Desktop\\Ayabe\\FMS\\CsvData\\2023\"\n",
    "files = glob.glob(csv_folder +\"*.csv\")              \n",
    "dfs = []\n",
    "for file in files:\n",
    "    i = pd.read_csv(file, encoding=\"cp932\", index = False)\n",
    "    dfs.append(i)\n",
    "df = pd.concat(dfs, axis=0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ea4cf4-7c5b-4707-a5e4-d549e9bc2700",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError reading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:347\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[0;32m    145\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    155\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    along the other axes.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:404\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    401\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "csv_folder=r\"C:\\Users\\00220401626\\Desktop\\Ayabe\\Conductivity\"\n",
    "files = glob.glob(csv_folder + \"*.csv\")\n",
    "dfs = []\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        i = pd.read_csv(file, encoding=\"cp932\", header=None)\n",
    "        dfs.append(i)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file}: {e}\")\n",
    "\n",
    "df = pd.concat(dfs, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cb7e06-ba5b-4776-9f68-18274706171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import ibm_db\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "import datetime as dt \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(filename='logfile.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "#########################################################################################################################################################################\n",
    "\n",
    "#0. データベースへの接続\n",
    "conn = ibm_db.connect(\"DATABASE=AYB_APPL;HOSTNAME=10.143.16.244;PORT=50000;PROTOCOL=TCPIP;UID=IOT_DATA;PWD=asd23fgh;\", \"\", \"\")\n",
    "\n",
    "if conn:\n",
    "    print(\"Connected to the database\")\n",
    "    logging.info(\"Connected to the database\")\n",
    "else:\n",
    "    print(\"Failed to connect to the database\")\n",
    "    logging.error(\"Failed to connect to the database\")\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "571ae474-ba82-40fc-8a3b-e83580ac05a5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database\n",
      "Latest CSV file: C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231022.csv\n",
      "Row 1 merged successfully\n",
      "Row 2 merged successfully\n",
      "Row 3 merged successfully\n",
      "Row 4 merged successfully\n",
      "Row 5 merged successfully\n",
      "Row 6 merged successfully\n",
      "Row 7 merged successfully\n",
      "Row 8 merged successfully\n",
      "Row 9 merged successfully\n",
      "Row 10 merged successfully\n",
      "Row 11 merged successfully\n",
      "Row 12 merged successfully\n",
      "Row 13 merged successfully\n",
      "Time spent: 0 minutes and 0 seconds\n",
      "\n",
      "\n",
      "Error: 'insertion_time' is 'nan' or not found in the JSON file.\n",
      "All CSV files below are going to be inserted into the database.\n",
      "\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231001.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231002.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231003.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231004.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231005.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231006.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231007.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231008.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231009.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231010.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231011.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231012.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231013.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231014.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231015.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231016.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231017.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231018.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231019.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231020.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231021.csv\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231022.csv\n",
      "\n",
      "\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231001.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231002.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231003.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231004.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231005.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231006.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231007.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231008.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231009.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231010.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231011.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231012.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231013.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231014.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231015.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231016.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231017.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231018.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231019.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231020.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231021.csv : Inserted successfully\n",
      "C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\\2026\\20231022.csv : Inserted successfully\n",
      "All CSV files are inserted\n",
      "Time spent: 5 minutes and 15 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import ibm_db\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "import datetime as dt \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(filename='logfile.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "#########################################################################################################################################################################\n",
    "\n",
    "#0. データベースへの接続\n",
    "conn = ibm_db.connect(\"DATABASE=AYB_APPL;HOSTNAME=10.143.16.244;PORT=50000;PROTOCOL=TCPIP;UID=IOT_DATA;PWD=asd23fgh;\", \"\", \"\")\n",
    "\n",
    "if conn:\n",
    "    print(\"Connected to the database\")\n",
    "    logging.info(\"Connected to the database\")\n",
    "else:\n",
    "    print(\"Failed to connect to the database\")\n",
    "    logging.error(\"Failed to connect to the database\")\n",
    "   \n",
    "\n",
    "##########################################################################################################################################################################\n",
    "#　　                                                          1) 各LOCATIONごとに最新のデータをINSERT/UPDATE  \n",
    "\n",
    "def function_1():\n",
    "    start_time = time.time()\n",
    "    #1.  最新フォルダの全CSVのPATHをまとめる。\n",
    "    directory = r\"C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\"\n",
    "    file_pattern = '20*'\n",
    "    files = glob.glob(os.path.join(directory, file_pattern))\n",
    "    files.sort()\n",
    "    \n",
    "    \n",
    "    #2. 最新のCSVファイルを探す。\n",
    "    if files:\n",
    "        folder_path = files[-1]\n",
    "        csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "    \n",
    "        if csv_files:\n",
    "            latest_csv_file = max(csv_files, key=os.path.getmtime)\n",
    "            df = pd.read_csv(latest_csv_file, encoding='cp932', dtype=object)\n",
    "            \n",
    "            \n",
    "    #3. Locationカラムから最新の値を変数に入れる。\n",
    "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "            unique_locations = df['Location'].unique()\n",
    "            latest_data = pd.DataFrame()\n",
    "            df_concat = []\n",
    "        \n",
    "            for location in unique_locations:\n",
    "                latest_location = df[df['Location'] == location].nlargest(1, 'Timestamp')\n",
    "                df_concat.append(latest_location)\n",
    "                \n",
    "            latest_data = pd.concat(df_concat, ignore_index=True)\n",
    "            print(\"Latest CSV file:\", latest_csv_file)\n",
    "    \n",
    "            logging.info(\"Latest CSV file: %s\", latest_csv_file)\n",
    "            logging.info(\"Processed data for %d locations\", len(unique_locations))\n",
    "            \n",
    "        else:\n",
    "            print(\"No CSV files found in the latest folder.\")\n",
    "            logging.warning(\"No CSV files found in the latest folder.\")\n",
    "    else:\n",
    "        print(\"No folders found.\")\n",
    "        logging.warning(\"No matching folders found.\")\n",
    "        \n",
    "     \n",
    "    # 4. データがあればUPDATE, データがなければINSERTする。\n",
    "    merge_query = \"\"\"\n",
    "    MERGE INTO LIQUID_PARTICLE_LAST AS target\n",
    "    USING (SELECT ? AS LOCATION, ? AS TS_DT, ? AS VALUE_1_0, ? AS VALUE_3_0, ? AS VALUE_5_0, ? AS VALUE_10_0, ? AS VALUE_15_0, ? AS VALUE_20_0, ? AS VALUE_25_0, ? AS VALUE_50_0 FROM SYSIBM.SYSDUMMY1) AS source\n",
    "    ON target.LOCATION = source.LOCATION\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET target.TS_DT = source.TS_DT, \n",
    "                   target.VALUE_1_0 = source.VALUE_1_0, \n",
    "                   target.VALUE_3_0 = source.VALUE_3_0, \n",
    "                   target.VALUE_5_0 = source.VALUE_5_0, \n",
    "                   target.VALUE_10_0 = source.VALUE_10_0, \n",
    "                   target.VALUE_15_0 = source.VALUE_15_0, \n",
    "                   target.VALUE_20_0 = source.VALUE_20_0, \n",
    "                   target.VALUE_25_0 = source.VALUE_25_0, \n",
    "                   target.VALUE_50_0 = source.VALUE_50_0\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (LOCATION, TS_DT, VALUE_1_0, VALUE_3_0, VALUE_5_0, VALUE_10_0, VALUE_15_0, VALUE_20_0, VALUE_25_0, VALUE_50_0)\n",
    "        VALUES (source.LOCATION, source.TS_DT, source.VALUE_1_0, source.VALUE_3_0, source.VALUE_5_0, source.VALUE_10_0, source.VALUE_15_0, source.VALUE_20_0, source.VALUE_25_0, source.VALUE_50_0);\n",
    "    \"\"\"\n",
    "    \n",
    "    stmt = ibm_db.prepare(conn, merge_query)\n",
    "    \n",
    "    for index, row in latest_data.iterrows():\n",
    "        location = row['Location']\n",
    "        timestamp = row['Timestamp']\n",
    "        c1 = row['1.0μｍ']\n",
    "        c3 = row['3.0μｍ']\n",
    "        c5 = row['5.0μｍ']\n",
    "        c10 = row['10.0μｍ']\n",
    "        c15 = row['15.0μｍ']\n",
    "        c20 = row['20.0μｍ']\n",
    "        c25 = row['25.0μｍ']\n",
    "        c50 = row['50.0μｍ']\n",
    "        \n",
    "        if ibm_db.execute(stmt, (location, timestamp, c1, c3, c5, c10, c15, c20, c25, c50)):\n",
    "            print(f\"Row {index + 1} merged successfully\")\n",
    "            logging.info(f\"Row {index + 1} merged successfully\")\n",
    "        else:\n",
    "            error_message = f\"Error merging row {index + 1}: {ibm_db.stmt_errormsg()}\"\n",
    "            print(error_message)\n",
    "            logging.error(error_message) \n",
    "   \n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    print(f\"Time spent: {minutes} minutes and {seconds} seconds\")\n",
    "    print('\\n') \n",
    "\n",
    "\n",
    "#####################################################################################################################################\n",
    "        \n",
    "# 　　　　　　　　　　　　　　　　　　　　　　　　2) JSON記録に記載された日より以降のデータをCSVファイルごとにINSERTする\n",
    "\n",
    "        \n",
    "#1.　JSON 記録から最後INSERTされた行の時間の取得。\n",
    "#2.　最新のフォルダから,全CSVのPATHをまとめる。\n",
    "#3.　JSONに記載された最後の行以降のデータをまとめる。  \n",
    "\n",
    "def function_2():\n",
    "    start_time = time.time()\n",
    "    json_file_path = 'insertion_time.json'\n",
    "       \n",
    "    if os.path.exists(json_file_path):\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            if 'insertion_time' in data and data['insertion_time'] and not pd.isna(data['insertion_time'])  :\n",
    "    \n",
    "                insertion_time_raw = str(data['insertion_time'])\n",
    "                inserted_time = dt.datetime.strptime(insertion_time_raw, '%Y-%m-%d %H:%M:%S')\n",
    "                inserted_day = inserted_time.strftime('%Y%m%d')\n",
    "                time_to_compare = insertion_time_raw\n",
    "                print(f\"Last Insertion time: {time_to_compare}\")\n",
    "                \n",
    "                directory = r\"C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\"\n",
    "                file_pattern = '20*'\n",
    "                files = glob.glob(os.path.join(directory, file_pattern))\n",
    "                files.sort()\n",
    "                folder_path = files[-1]\n",
    "                csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "                selected_csv_files = [file for file in csv_files if int(file[-12:-4]) >= int(inserted_day)]\n",
    "               \n",
    "            else:\n",
    "                print(\"Error: 'insertion_time' is 'nan' or not found in the JSON file.\")\n",
    "                logging.error(\"Error: 'insertion_time' is 'nan' or not found in the JSON file.\")\n",
    "                directory = r\"C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\"\n",
    "                file_pattern = '20*'\n",
    "                files = glob.glob(os.path.join(directory, file_pattern))\n",
    "                files.sort()\n",
    "                folder_path = files[-1]\n",
    "                selected_csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "                time_to_compare = False\n",
    "         \n",
    "    else:\n",
    "        print(\"JSON file does not exist. \\nAll CSV files below are going to be inserted into the database.\")\n",
    "        logging.info(\"JSON file does not exist. All CSV files below are going to be inserted into the database.\")\n",
    "        directory = r\"C:\\Users\\00220401626\\Desktop\\FMS\\CsvData\"\n",
    "        file_pattern = '20*'\n",
    "        files = glob.glob(os.path.join(directory, file_pattern))\n",
    "        files.sort()\n",
    "        folder_path = files[-1]\n",
    "        selected_csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "        print(selected_csv_files)\n",
    "        time_to_compare = False\n",
    "    print('All CSV files below are going to be inserted into the database.\\n') \n",
    "    logging.info('All CSV files below are going to be inserted into the database.\\n')\n",
    "    for i in selected_csv_files:\n",
    "        print(i)\n",
    "     \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "# 4.JSONに記載された最後の行より以降のデータのcsvを１つずつINSERTする\n",
    "    if selected_csv_files:\n",
    "        \n",
    "        for i in selected_csv_files:\n",
    "            df = pd.read_csv(fr\"{i}\", encoding='cp932', dtype= object)\n",
    "            df['Timestamp'] = df['Timestamp'].str.replace('/', '-', regex=False)\n",
    "    \n",
    "            if os.path.exists(json_file_path) and time_to_compare:\n",
    "                a = df[df['Timestamp'] >  time_to_compare]\n",
    "                data = tuple(tuple(row) for row in a.values )\n",
    "                   \n",
    "                b = \"\"\n",
    "                for row in data:\n",
    "                    b += str(row) +\",\"\n",
    "                b = b[:-1]\n",
    "                \n",
    "                insert_query = f\"INSERT INTO LIQUID_PARTICLE_TEST (Location, TS_DT, VALUE_1_0, VALUE_3_0, VALUE_5_0, VALUE_10_0, VALUE_15_0, VALUE_20_0, VALUE_25_0, VALUE_50_0) VALUES {b}\"\n",
    "                stmt = ibm_db.prepare(conn, insert_query)\n",
    "                \n",
    "                try:\n",
    "                    if ibm_db.execute(stmt, b):\n",
    "                        print(f\"{i} : Inserted successfully\") \n",
    "                        logging.info(f\"{i} : Inserted successfully\")\n",
    "                except:\n",
    "                    print(f\"({i}) is already updated\")\n",
    "                    logging.info(f\"{i} : is already updated\")\n",
    "    \n",
    "                #5.時間を JSON ファイルに保存する     \n",
    "                insertion_time = a['Timestamp'].max()\n",
    "                json_data = {'insertion_time': insertion_time}\n",
    "                with open('insertion_time.json', 'w') as json_file:\n",
    "                    json.dump(json_data, json_file, indent=4)\n",
    "            else:\n",
    "                data = tuple(tuple(row) for row in df.values )\n",
    "                   \n",
    "                b = \"\"\n",
    "                for row in data:\n",
    "                    b += str(row) +\",\"\n",
    "                b = b[:-1]\n",
    "                \n",
    "                insert_query = f\"INSERT INTO LIQUID_PARTICLE_TEST (Location, TS_DT, VALUE_1_0, VALUE_3_0, VALUE_5_0, VALUE_10_0, VALUE_15_0, VALUE_20_0, VALUE_25_0, VALUE_50_0) VALUES {b}\"\n",
    "                stmt = ibm_db.prepare(conn, insert_query)\n",
    "                \n",
    "                try:\n",
    "                    if ibm_db.execute(stmt, b):\n",
    "                        print(f\"{i} : Inserted successfully\") \n",
    "                        logging.info(f\"{i} : Inserted successfully\")\n",
    "                except:\n",
    "                    print(f\"({i}) is already updated\")\n",
    "                    logging.info(f\"{i} : is already updated\")\n",
    "                    \n",
    "            \n",
    "                #5.時間を JSON ファイルに保存する          \n",
    "                insertion_time = df['Timestamp'].max()\n",
    "                json_data = {'insertion_time': insertion_time}\n",
    "                with open('insertion_time.json', 'w') as json_file:\n",
    "                    json.dump(json_data, json_file, indent=4) \n",
    "\n",
    "                    \n",
    "        print('All CSV files are inserted')\n",
    "        logging.info('All CSV files are inserted')\n",
    "        \n",
    "    elif not selected_csv_files:\n",
    "        print(\"Database is already updated\")\n",
    "        logging.info(\"Database is already updated\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    print(f\"Time spent: {minutes} minutes and {seconds} seconds\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    function_1()\n",
    "    function_2()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
